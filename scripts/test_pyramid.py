#!/usr/bin/env python3
"""
Generate test pyramid visualization.

Shows breakdown of tests by type for each module.
"""

import re
import subprocess
from collections import defaultdict
from pathlib import Path


def count_tests_by_marker(path: Path, marker: str) -> int:
    """Count tests with specific marker in path."""
    try:
        result = subprocess.run(
            ["pytest", str(path), "-m", marker, "--collect-only", "-q"],
            capture_output=True,
            text=True,
            check=False,  # Don't raise on non-zero exit
        )

        # Check for errors
        if result.returncode != 0 and result.returncode != 5:  # 5 = no tests collected
            print(f"Warning: pytest error for {path} with marker {marker}: {result.stderr}")
            return 0

        # Parse output like "15 tests collected"
        match = re.search(r"(\d+) tests? collected", result.stdout)
        return int(match.group(1)) if match else 0
    except FileNotFoundError:
        print("Error: pytest not found. Make sure pytest is installed.")
        return 0
    except Exception as e:
        print(f"Error counting tests in {path} with marker {marker}: {e}")
        return 0


def count_total_tests(path: Path) -> int:
    """Count all tests in path."""
    try:
        result = subprocess.run(
            ["pytest", str(path), "--collect-only", "-q"],
            capture_output=True,
            text=True,
            check=False,  # Don't raise on non-zero exit
        )

        # Check for errors
        if result.returncode != 0 and result.returncode != 5:  # 5 = no tests collected
            print(f"Warning: pytest error for {path}: {result.stderr}")
            return 0

        # Parse output like "15 tests collected"
        match = re.search(r"(\d+) tests? collected", result.stdout)
        return int(match.group(1)) if match else 0
    except FileNotFoundError:
        print("Error: pytest not found. Make sure pytest is installed.")
        return 0
    except Exception as e:
        print(f"Error counting tests in {path}: {e}")
        return 0


def generate_pyramid():
    """Generate test pyramid visualization."""

    modules = {
        "AD (Differentiation)": "tests/unit/ad",
        "GAMS Parser": "tests/unit/gams",
        "IR": "tests/unit/ir",
        "KKT": "tests/unit/kkt",
        "Emit": "tests/unit/emit",
        "Integration (KKT)": "tests/integration/kkt",
        "Integration (AD)": "tests/integration/ad",
        "Integration (GAMS-IR)": "tests/integration/gams_ir",
        "End-to-End": "tests/e2e",
    }

    results = {}
    for name, path in modules.items():
        path_obj = Path(path)
        if not path_obj.exists():
            continue

        results[name] = {
            "unit": count_tests_by_marker(path_obj, "unit"),
            "integration": count_tests_by_marker(path_obj, "integration"),
            "e2e": count_tests_by_marker(path_obj, "e2e"),
            "total": count_total_tests(path_obj),
        }

    # Generate markdown
    md = []
    md.append("# Test Pyramid Visualization")
    md.append("")
    md.append("*Auto-generated by `scripts/test_pyramid.py`*")
    md.append("")
    md.append("## Overview")
    md.append("")
    md.append("```")
    md.append("        /\\")
    md.append("       /  \\  E2E (slowest, full pipeline)")
    md.append("      /    \\")
    md.append("     /------\\")
    md.append("    /        \\  Integration (medium, cross-module)")
    md.append("   /----------\\")
    md.append("  /            \\")
    md.append(" /--------------\\")
    md.append("/________________\\  Unit (fastest, isolated)")
    md.append("```")
    md.append("")
    md.append("## Test Counts by Module")
    md.append("")
    md.append("| Module | Unit | Integration | E2E | Total |")
    md.append("|--------|------|-------------|-----|-------|")

    totals = defaultdict(int)
    for name in sorted(results.keys()):
        counts = results[name]
        md.append(
            f"| {name} | {counts['unit']} | {counts['integration']} | "
            f"{counts['e2e']} | {counts['total']} |"
        )
        for key in ["unit", "integration", "e2e", "total"]:
            totals[key] += counts[key]

    md.append(
        f"| **TOTAL** | **{totals['unit']}** | **{totals['integration']}** | "
        f"**{totals['e2e']}** | **{totals['total']}** |"
    )
    md.append("")

    # Add pyramid visualization
    md.append("## Pyramid Balance")
    md.append("")
    md.append("Ideal test pyramid: Many unit tests, fewer integration, even fewer e2e.")
    md.append("")

    if totals["total"] > 0:
        md.append(f"- Unit: {totals['unit']} ({100 * totals['unit'] // totals['total']}%)")
        md.append(
            f"- Integration: {totals['integration']} ({100 * totals['integration'] // totals['total']}%)"
        )
        md.append(f"- E2E: {totals['e2e']} ({100 * totals['e2e'] // totals['total']}%)")
    else:
        md.append("- No tests found")

    md.append("")

    # Check balance
    if totals["unit"] < totals["integration"]:
        md.append("⚠️ **Warning:** More integration than unit tests. Add more unit tests.")
    elif totals["unit"] > 2 * totals["integration"]:
        md.append("✅ **Good:** Healthy test pyramid with strong unit test base.")
    else:
        md.append("✅ **Acceptable:** Reasonable balance between unit and integration tests.")

    md.append("")
    md.append("## Test Execution Speed")
    md.append("")
    md.append("Expected execution times (estimated):")
    if totals["total"] > 0:
        md.append(f"- Unit only: ~{totals['unit'] * 0.01:.1f}s (fast feedback)")
        md.append(
            f"- Unit + Integration: ~{(totals['unit'] + totals['integration']) * 0.05:.1f}s (pre-commit)"
        )
        md.append(f"- Full suite: ~{totals['total'] * 0.1:.1f}s (CI)")
    md.append("")

    # Add coverage gaps section
    md.append("## Module Coverage Analysis")
    md.append("")
    md.append("Modules with test coverage:")
    for name in sorted(results.keys()):
        counts = results[name]
        if counts["total"] > 0:
            coverage_levels = []
            if counts["unit"] > 0:
                coverage_levels.append("Unit")
            if counts["integration"] > 0:
                coverage_levels.append("Integration")
            if counts["e2e"] > 0:
                coverage_levels.append("E2E")
            coverage_str = ", ".join(coverage_levels) if coverage_levels else "Untagged"
            md.append(f"- **{name}**: {coverage_str} ({counts['total']} tests)")

    md.append("")
    md.append("---")
    md.append("")
    md.append("*Last updated: Auto-generated*")
    md.append("")

    return "\n".join(md)


if __name__ == "__main__":
    markdown = generate_pyramid()

    output_path = Path("docs/testing/TEST_PYRAMID.md")
    output_path.parent.mkdir(exist_ok=True, parents=True)
    output_path.write_text(markdown)

    print(f"✅ Test pyramid generated: {output_path}")
    print()
    print(markdown)
