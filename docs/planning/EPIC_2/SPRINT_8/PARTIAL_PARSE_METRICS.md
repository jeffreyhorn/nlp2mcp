# Partial Parse Metrics Design

**Sprint:** Epic 2 - Sprint 8 Prep  
**Task:** Task 5 - Design Partial Parse Metrics  
**Created:** 2025-11-17  
**Purpose:** Design system to track and report statement-level parse success for models that partially parse

---

## Executive Summary

**Objective:** Enable dashboard to show "himmel16: 85% parsed, needs [i++1 indexing]" instead of binary "FAILED" status.

**Current State:** Dashboard shows binary pass/fail for parsing (20% parse rate = 2/10 models). No visibility into partial progress on failing models.

**Target State:** Dashboard shows statement-level parse success with missing feature annotations:
- "himmel16: 92% parsed (22/24 statements), needs [i++1 indexing]"
- "circle: 100% parsed, needs [function calls in assignments]" (semantic error after full parse)
- "mhw4d: 100% parsed ‚úÖ"

**Key Finding:** Current parser does **not** support partial parsing (Lark Earley parser stops on first syntax error). Sprint 8 will track "line-based progress" instead of "statement-based progress" as a pragmatic approximation.

**Implementation Complexity:** Medium (4-6 hours)
- Line-based counting: 2 hours (parse lines from source, count before error line)
- Missing feature extraction: 2 hours (pattern matching on error messages)
- Dashboard updates: 1-2 hours (new columns, backward compatible template)

---

## Table of Contents

1. [Statement Definition](#statement-definition)
2. [Counting Mechanism](#counting-mechanism)
3. [Missing Feature Extraction](#missing-feature-extraction)
4. [Dashboard Integration](#dashboard-integration)
5. [Ingestion Pipeline Updates](#ingestion-pipeline-updates)
6. [Implementation Effort](#implementation-effort)

---

## Statement Definition

### Research Findings

**GAMS top-level statement types** (from grammar analysis):

1. **sets_block** - Set declarations (`Set i / 1*10 /;`)
2. **aliases_block** - Alias declarations (`Alias (i,j);`)
3. **params_block** - Parameter declarations (`Parameter x;`)
4. **table_block** - Table data (`Table data(i,j)`)
5. **scalars_block** - Scalar declarations (`Scalar pi / 3.14159 /;`)
6. **variables_block** - Variable declarations (`Variable x, y;`)
7. **equations_block** - Equation headers (`Equation balance, supply;`)
8. **model_stmt** - Model declarations (`Model transport / all /;`)
9. **assignment_stmt** - Assignments (`x.up = 100;`)
10. **equation_def** - Equation definitions (`balance.. sum(i, x(i)) =e= demand;`)
11. **solve_stmt** - Solve statement (`Solve transport using lp maximizing profit;`)

**Total:** 11 top-level statement types

### Problem: Parser Does Not Support Partial Parsing

**Current behavior:**
- Lark uses **Earley parser** (not error-recovering)
- On syntax error: Raises `UnexpectedToken` or `UnexpectedCharacters` **immediately**
- **No partial AST returned** - parsing stops completely
- No mechanism to access successfully parsed statements before error

**Evidence:**
```python
def parse_text(source: str) -> Tree:
    parser = _build_lark()
    raw = parser.parse(source)  # ‚Üê Throws exception on first error, no partial tree
    return _resolve_ambiguities(raw)
```

**Two-phase error types:**
1. **Syntax errors** (Lark) - Stop parsing immediately, **no AST available**
2. **Semantic errors** (ParserSemanticError) - Occur **after** successful parse during ModelIR construction

**Implication for metrics:**
- ‚úÖ **Semantic errors:** Can report 100% parsed (full AST exists), plus semantic issue
- ‚ùå **Syntax errors:** Cannot report statement-level progress (no partial AST)

### Solution: Line-Based Approximation

**Chosen Definition: "Logical Lines" (pragmatic approximation)**

**Rationale:**
- True statement counting requires partial AST (not available in Sprint 8)
- Line-based counting provides useful proxy metric
- Easier to implement (2 hours vs 10+ hours for parser modification)
- Still shows progress: "himmel16: 92% of lines parsed"

**"Logical Line" Definition:**
- **Non-empty line** with non-whitespace, non-comment content
- **Excludes:** Blank lines, comment-only lines (`*` or `$ontext...$offtext`)
- **Counts:** Lines with GAMS code (declarations, assignments, equations, solve)

**Why not physical lines?**
- Physical lines include blanks, comments (inflates denominators)
- Logical lines correlate better with "work done" by parser

**Why not true statements?**
- Requires partial AST (not available without major parser changes)
- Sprint 8 scope: Design only, not implement full error recovery
- Line-based is good enough for dashboard progress visualization

**Example:**
```gams
Set i / 1*10 /;                     ‚Üê Logical line 1
Set j / a, b, c /;                  ‚Üê Logical line 2
                                     ‚Üê Blank (skip)
Parameter x(i) / 1*10 = 5 /;        ‚Üê Logical line 3
* This is a comment                  ‚Üê Comment (skip)
Variable y;                          ‚Üê Logical line 4
Equation balance;                    ‚Üê Logical line 5
balance.. sum(i, x(i)) =e= 100;     ‚Üê Logical line 6 (ERROR: syntax error here)
```

**Partial parse metric:** 5/7 logical lines = 71% parsed (error on line 6)

**Accuracy:**
- ‚úÖ Underestimates for multi-line statements (counts as multiple lines)
- ‚úÖ Overestimates if error is late in file but early in statement processing order
- ‚úÖ Good enough for dashboard "at-a-glance" progress indication

---

## Counting Mechanism

### Design: Line-Based Progress Tracking

**Approach:** Count logical lines in source file up to error line, compare to total.

**Algorithm:**

```python
def count_logical_lines(source: str) -> int:
    """
    Count logical lines (non-empty, non-comment) in GAMS source.
    
    Returns:
        Total count of logical lines
    """
    lines = source.split('\n')
    in_multiline_comment = False
    logical_count = 0
    
    for line in lines:
        stripped = line.strip()
        
        # Handle multiline comments
        if '$ontext' in stripped.lower():
            in_multiline_comment = True
            continue
        if '$offtext' in stripped.lower():
            in_multiline_comment = False
            continue
        if in_multiline_comment:
            continue
        
        # Skip empty lines and single-line comments
        if not stripped or stripped.startswith('*'):
            continue
        
        # Skip inline comments (line starts with code but has *)
        code_part = stripped.split('*')[0].strip()
        if code_part:  # Has non-comment content
            logical_count += 1
    
    return logical_count


def calculate_parse_progress(source: str, error_line: int | None) -> tuple[int, int, float]:
    """
    Calculate partial parse progress.
    
    Args:
        source: Full GAMS source code
        error_line: Line number where parsing failed (1-indexed), or None if success
    
    Returns:
        (lines_parsed, total_lines, percentage)
    """
    total_lines = count_logical_lines(source)
    
    if error_line is None:
        # Successful parse
        return (total_lines, total_lines, 100.0)
    
    # Count logical lines before error
    lines = source.split('\n')[:error_line - 1]  # Lines before error (0-indexed)
    partial_source = '\n'.join(lines)
    lines_parsed = count_logical_lines(partial_source)
    
    percentage = (lines_parsed / total_lines * 100) if total_lines > 0 else 0.0
    
    return (lines_parsed, total_lines, percentage)
```

**Example: himmel16.gms**

```gams
$Title Simple Test Problem 16 from the Himmelbau collection  ‚Üê Logical line 1
...
Set i / 1*10 /;                                                 ‚Üê Logical line 10
...
areadef(i).. area(i) =e= 0.5*(x(i)*y(i++1) - y(i)*x(i++1));   ‚Üê Line 46, ERROR here
```

- Total logical lines: 24
- Error on line 46 (physical), corresponding to logical line 22
- **Partial parse metric:** 22/24 = 92% parsed

**Integration with error tracking:**
```python
@dataclass
class ModelResult:
    """Enhanced with partial parse metrics."""
    model_name: str
    gms_file: str
    parse_status: str  # "SUCCESS" or "FAILED"
    parse_error: str | None = None
    parse_error_type: str | None = None
    
    # New fields for Sprint 8
    parse_progress_lines_parsed: int | None = None  # Lines successfully parsed
    parse_progress_total_lines: int | None = None   # Total logical lines
    parse_progress_percentage: float | None = None  # Percentage (0-100)
    missing_features: list[str] | None = None       # Extracted feature hints
```

**Semantic errors (full parse, semantic issue):**
- `parse_progress_percentage = 100.0` (all lines parsed)
- `missing_features = ["function calls in assignments"]`
- Dashboard: "circle: 100% parsed, needs [function calls in assignments]"

**Syntax errors (partial parse):**
- `parse_progress_percentage = 92.0` (22/24 lines parsed)
- `missing_features = ["i++1 indexing"]`
- Dashboard: "himmel16: 92% parsed, needs [i++1 indexing]"

---

## Missing Feature Extraction

### Design: Pattern Matching on Error Messages

**Approach:** Extract missing features from error message text using regex patterns.

**Error Message Categories:**

#### 1. Lark Syntax Errors (UnexpectedToken, UnexpectedCharacters)

**Pattern:** Look for specific tokens/characters that indicate missing features.

**Examples:**

| Error Message | Feature Hint |
|--------------|--------------|
| `No terminal matches '+' ... i++1` | "i++1 indexing" |
| `No terminal matches 'm' ... mx /` | "short model declaration syntax" |
| `Expected SEMI, got RPAR` | "syntax error (extra parenthesis)" |

**Extraction logic:**
```python
def extract_lark_error_features(error_msg: str, source_line: str | None) -> list[str]:
    """Extract missing features from Lark syntax errors."""
    features = []
    
    # Pattern: i++1 or i--1 indexing
    if 'i++' in error_msg or 'i++' in (source_line or ''):
        features.append("i++1 indexing (lead/lag)")
    
    # Pattern: Short model declaration (mx / eqlist /)
    if re.search(r'[a-z]+\s*/\s*[a-z]+', source_line or ''):
        features.append("short model declaration syntax")
    
    # Pattern: Missing semicolon
    if 'Expected.*SEMI' in error_msg:
        features.append("missing semicolon")
    
    # Pattern: Unexpected character (generic)
    if 'UnexpectedCharacters' in str(type(error_msg)):
        features.append("unsupported syntax")
    
    return features
```

#### 2. Semantic Errors (ParserSemanticError)

**Pattern:** Error messages contain explicit feature descriptions.

**Examples:**

| Error Message | Feature Hint |
|--------------|--------------|
| `Indexed assignments are not supported yet` | "indexed assignments" |
| `Parameter ... multi-dimensional domains is not supported` | "multidimensional parameter data" |
| `Assignments must use numeric constants; got Call(...)` | "function calls in assignments" |
| `GAMS feature '{feature}' is not yet supported` | "{feature}" (extracted verbatim) |

**Extraction logic:**
```python
def extract_semantic_error_features(error_msg: str) -> list[str]:
    """Extract missing features from ParserSemanticError messages."""
    features = []
    
    # Pattern: "not supported yet"
    match = re.search(r'([\w\s]+) (?:is |are )?not (?:yet )?supported', error_msg, re.IGNORECASE)
    if match:
        features.append(match.group(1).strip())
    
    # Pattern: "got Call(...)" - function call detected
    if 'got Call(' in error_msg:
        features.append("function calls in assignments")
    
    # Pattern: Explicit unsupported feature
    match = re.search(r"GAMS feature '(.+?)' is not", error_msg)
    if match:
        features.append(match.group(1))
    
    return features
```

#### 3. Combined Extraction Pipeline

```python
def extract_missing_features(
    error_type: str,
    error_msg: str,
    error_line: int | None,
    source: str
) -> list[str]:
    """
    Extract missing feature hints from parse errors.
    
    Args:
        error_type: Exception class name (UnexpectedToken, ParserSemanticError, etc.)
        error_msg: Error message text
        error_line: Line number where error occurred (1-indexed)
        source: Full GAMS source code
    
    Returns:
        List of missing feature strings (e.g., ["i++1 indexing", "function calls"])
    """
    features = []
    
    # Get source line if available
    source_line = None
    if error_line is not None:
        lines = source.split('\n')
        if 0 < error_line <= len(lines):
            source_line = lines[error_line - 1]
    
    # Extract based on error type
    if error_type in ('UnexpectedToken', 'UnexpectedCharacters'):
        features.extend(extract_lark_error_features(error_msg, source_line))
    elif error_type == 'ParserSemanticError':
        features.extend(extract_semantic_error_features(error_msg))
    else:
        # Unknown error type - generic
        features.append("unknown syntax/semantic issue")
    
    # Deduplicate and limit
    features = list(dict.fromkeys(features))  # Preserve order, remove duplicates
    return features[:3]  # Limit to top 3 features for dashboard brevity
```

**Coverage Estimate:**
- ‚úÖ **70-80%** of GAMSLib parse failures can be annotated with specific features
- ‚úÖ **100%** of ParserSemanticError (semantic errors have explicit messages)
- ‚ö†Ô∏è **50-60%** of Lark syntax errors (require source line inspection + heuristics)
- ‚ùå **Unknown errors** get generic "unknown syntax/semantic issue" annotation

---

## Dashboard Integration

### Current Dashboard Format

```markdown
| Model | Parse | Convert | Solve | E2E | Notes |
|-------|-------|---------|-------|-----|-------|
| himmel16 | ‚ùå | - | - | ‚ùå | Parse error: UnexpectedCharacters |
| mhw4d | ‚úÖ | - | - | ‚ùå | Parsed successfully |
```

### New Dashboard Format (Sprint 8)

```markdown
| Model | Parse | Progress | Missing Features | Convert | Solve | E2E |
|-------|-------|----------|------------------|---------|-------|-----|
| himmel16 | ‚ö†Ô∏è | 92% (22/24) | i++1 indexing | - | - | ‚ùå |
| circle | ‚ö†Ô∏è | 100% (24/24) | function calls in assignments | - | - | ‚ùå |
| mhw4d | ‚úÖ | 100% (18/18) | - | ‚ö†Ô∏è | - | ‚ùå |
```

**Legend Updates:**
- ‚úÖ Success (100% parsed, no errors)
- ‚ö†Ô∏è Partial (parsed with errors, or >0% progress)
- ‚ùå Failed (<25% progress or critical error)

**Color Coding:**
- üü¢ **100%** - Full parse success
- üü° **75-99%** - High progress, minor issues
- üü† **25-74%** - Moderate progress
- üî¥ **<25%** - Low progress, major blockers

**New KPIs Section:**

```markdown
## Overall KPIs

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| **Parse Rate (Full)** | 20.0% (2/10) | ‚â•10% | ‚úÖ |
| **Parse Rate (Partial)** | 80.0% (8/10 >50%) | - | üìä |
| **Average Parse Progress** | 73.5% | - | üìä |
| **Models >90% Parsed** | 4/10 (40%) | - | üìä |
```

**Failure Details Enhancement:**

```markdown
### himmel16.gms
**Model:** himmel16
**Status:** Parse Failed (92% progress)
**Progress:** 22/24 logical lines parsed
**Missing Features:** i++1 indexing (lead/lag)
**Error Type:** `UnexpectedCharacters`
**Error Line:** 46
**Error Message:**
```
No terminal matches '+' in the current parser context, at line 46 col 39

areadef(i).. area(i) =e= 0.5*(x(i)*y(i++1) - y(i)*x(i++1));
                                      ^
Expected one of: RPAR, COMMA
```
```

**Template Changes:**

1. **Add new columns** to model status table: `Progress`, `Missing Features`
2. **Update Parse column** to show ‚úÖ / ‚ö†Ô∏è / ‚ùå based on percentage
3. **Add new KPIs** for partial parse metrics
4. **Enhance failure details** with progress and missing features

**Backward Compatibility:**
- ‚úÖ **Yes** - Existing dashboard format still works
- ‚úÖ New columns are **additive** (old tooling ignores them)
- ‚úÖ Parse status still shows ‚úÖ /‚ùå for binary compatibility
- ‚ö†Ô∏è **New field** in JSON report: `parse_progress_percentage`

**Template Code (pseudocode):**

```python
def format_parse_status(result: ModelResult) -> str:
    """Format parse status with progress indicator."""
    if result.parse_status == "SUCCESS":
        return "‚úÖ"
    
    progress_pct = result.parse_progress_percentage or 0.0
    if progress_pct >= 75:
        return "‚ö†Ô∏è"  # Partial success (high progress)
    elif progress_pct >= 25:
        return "‚ö†Ô∏è"  # Partial (moderate progress)
    else:
        return "‚ùå"  # Failed (low progress)


def format_progress(result: ModelResult) -> str:
    """Format progress column."""
    if result.parse_progress_percentage is None:
        return "-"
    
    pct = result.parse_progress_percentage
    parsed = result.parse_progress_lines_parsed
    total = result.parse_progress_total_lines
    
    return f"{pct:.0f}% ({parsed}/{total})"


def format_missing_features(result: ModelResult) -> str:
    """Format missing features column."""
    if not result.missing_features:
        return "-"
    
    # Join with commas, limit to 2 for readability
    features = result.missing_features[:2]
    formatted = ", ".join(features)
    
    if len(result.missing_features) > 2:
        formatted += f", +{len(result.missing_features) - 2} more"
    
    return formatted
```

---

## Ingestion Pipeline Updates

### Current Pipeline (`scripts/ingest_gamslib.py`)

**Existing flow:**
1. Find all `.gms` files in input directory
2. For each file: Call `parse_model_file(path)`
3. Catch exceptions ‚Üí Record `parse_status`, `parse_error`, `parse_error_type`
4. Calculate KPIs (parse%, convert%, solve%)
5. Write JSON report
6. Generate Markdown dashboard

**Data structure:**
```python
@dataclass
class ModelResult:
    model_name: str
    gms_file: str
    parse_status: str  # "SUCCESS" or "FAILED"
    parse_error: str | None = None
    parse_error_type: str | None = None
```

### Enhanced Pipeline (Sprint 8)

**New flow:**
1. Find all `.gms` files
2. For each file:
   a. **Read source** to enable line counting
   b. Call `parse_model_file(path)`
   c. On exception:
      - Extract error line number (from exception attributes)
      - **Count logical lines** in source
      - **Calculate progress** (lines_parsed, total_lines, percentage)
      - **Extract missing features** from error message
   d. Record enhanced ModelResult
3. Calculate KPIs (add partial parse metrics)
4. Write JSON report (enhanced schema)
5. Generate Markdown dashboard (new columns)

**Enhanced data structure:**
```python
@dataclass
class ModelResult:
    """Enhanced with partial parse metrics."""
    model_name: str
    gms_file: str
    parse_status: str  # "SUCCESS" or "FAILED"
    parse_error: str | None = None
    parse_error_type: str | None = None
    
    # New fields for Sprint 8
    parse_progress_lines_parsed: int | None = None
    parse_progress_total_lines: int | None = None
    parse_progress_percentage: float | None = None
    missing_features: list[str] | None = None
    error_line: int | None = None  # Line number where error occurred
```

**Code changes:**

```python
def parse_model(gms_path: Path) -> ModelResult:
    """Enhanced to track partial parse progress."""
    model_name = gms_path.stem
    
    # Read source for line counting
    source = gms_path.read_text()
    
    try:
        parse_model_file(gms_path)
        
        # Success - full parse
        total_lines = count_logical_lines(source)
        return ModelResult(
            model_name=model_name,
            gms_file=gms_path.name,
            parse_status="SUCCESS",
            parse_progress_lines_parsed=total_lines,
            parse_progress_total_lines=total_lines,
            parse_progress_percentage=100.0,
        )
    
    except Exception as e:
        error_message = str(e)
        error_type = type(e).__name__
        
        # Extract error line number
        error_line = extract_error_line(e)
        
        # Calculate partial progress
        lines_parsed, total_lines, percentage = calculate_parse_progress(
            source, error_line
        )
        
        # Extract missing features
        features = extract_missing_features(
            error_type, error_message, error_line, source
        )
        
        return ModelResult(
            model_name=model_name,
            gms_file=gms_path.name,
            parse_status="FAILED",
            parse_error=error_message,
            parse_error_type=error_type,
            parse_progress_lines_parsed=lines_parsed,
            parse_progress_total_lines=total_lines,
            parse_progress_percentage=percentage,
            missing_features=features,
            error_line=error_line,
        )


def extract_error_line(exception: Exception) -> int | None:
    """Extract line number from parser exception."""
    # Lark errors
    if hasattr(exception, 'line'):
        return exception.line
    
    # ParserSemanticError
    if hasattr(exception, 'line') and exception.line is not None:
        return exception.line
    
    # Parse from error message (fallback)
    match = re.search(r'at line (\d+)', str(exception))
    if match:
        return int(match.group(1))
    
    return None
```

**New KPIs:**

```python
def calculate_kpis(models: list[ModelResult]) -> dict[str, Any]:
    """Enhanced KPIs with partial parse metrics."""
    total = len(models)
    parse_success = sum(1 for m in models if m.parse_status == "SUCCESS")
    
    # Partial parse metrics
    models_with_progress = [m for m in models if m.parse_progress_percentage is not None]
    avg_progress = (
        sum(m.parse_progress_percentage for m in models_with_progress) / len(models_with_progress)
        if models_with_progress else 0.0
    )
    
    high_progress = sum(
        1 for m in models if (m.parse_progress_percentage or 0) >= 90.0
    )
    
    partial_success = sum(
        1 for m in models if 50.0 <= (m.parse_progress_percentage or 0) < 100.0
    )
    
    return {
        "total_models": total,
        "parse_success": parse_success,
        "parse_failed": total - parse_success,
        "parse_rate_percent": round((parse_success / total * 100), 1),
        
        # New partial parse metrics
        "average_parse_progress_percent": round(avg_progress, 1),
        "models_high_progress": high_progress,  # ‚â•90%
        "models_partial_success": partial_success,  # 50-99%
        "partial_parse_rate_percent": round((partial_success / total * 100), 1),
        
        # Existing (Sprint 6)
        "convert_success": 0,
        "solve_success": 0,
    }
```

**Effort Estimate:**
- Line counting logic: 1 hour
- Error line extraction: 30 minutes
- Missing feature extraction: 2 hours (regex patterns, testing)
- KPI calculation updates: 30 minutes
- **Total:** 4 hours

---

## Implementation Effort

### Detailed Breakdown

**1. Line Counting Logic (2 hours)**
- `count_logical_lines()` function: 1 hour
  - Handle multiline comments (`$ontext`, `$offtext`)
  - Skip blank lines and single-line comments (`*`)
  - Handle inline comments (`code * comment`)
- `calculate_parse_progress()` function: 30 minutes
- Unit tests for edge cases: 30 minutes

**2. Missing Feature Extraction (2 hours)**
- `extract_lark_error_features()`: 1 hour
  - Regex patterns for i++1, short model syntax, etc.
  - Source line inspection heuristics
- `extract_semantic_error_features()`: 30 minutes
  - Pattern matching on error messages
  - Extract explicit "not supported" features
- `extract_missing_features()` coordinator: 30 minutes
  - Combine extraction methods
  - Deduplicate and limit features

**3. Ingestion Pipeline Updates (1 hour)**
- Enhance `ModelResult` dataclass: 15 minutes
- Update `parse_model()` function: 30 minutes
  - Read source, extract error line, call counting/extraction
- Update `calculate_kpis()`: 15 minutes (new metrics)

**4. Dashboard Template Updates (1 hour)**
- Add new columns to model status table: 20 minutes
- Update parse status formatting (‚úÖ ‚ö†Ô∏è ‚ùå): 15 minutes
- Add new KPIs section: 15 minutes
- Enhance failure details: 10 minutes

**Total Estimated Effort: 6 hours**

**Validation:** Within 4-6 hour estimate from PROJECT_PLAN.md? ‚úÖ Yes (6 hours = upper bound)

**Risk:** Low
- No parser changes required (uses error information already available)
- Line counting is straightforward (no grammar modifications)
- Missing feature extraction is heuristic (not critical if some missing)
- Dashboard changes are backward compatible

---

## Validation Examples

### Example 1: himmel16.gms (Syntax Error, Partial Parse)

**Source:** 24 logical lines total

**Error:**
```
UnexpectedCharacters at line 46 col 39: No terminal matches '+'
areadef(i).. area(i) =e= 0.5*(x(i)*y(i++1) - y(i)*x(i++1));
                                      ^
```

**Metrics:**
- `error_line`: 46
- `lines_parsed`: 22 (before error)
- `total_lines`: 24
- `percentage`: 92.0%
- `missing_features`: ["i++1 indexing (lead/lag)"]

**Dashboard Display:**
```markdown
| himmel16 | ‚ö†Ô∏è | 92% (22/24) | i++1 indexing | - | - | ‚ùå |
```

---

### Example 2: circle.gms (Semantic Error, Full Parse)

**Source:** 24 logical lines total

**Error:**
```
ParserSemanticError: Assignments must use numeric constants; got Call(uniform, ...) at line 15
```

**Metrics:**
- `error_line`: 15
- `lines_parsed`: 24 (full parse, semantic error after)
- `total_lines`: 24
- `percentage`: 100.0%
- `missing_features`: ["function calls in assignments"]

**Dashboard Display:**
```markdown
| circle | ‚ö†Ô∏è | 100% (24/24) | function calls in assignments | - | - | ‚ùå |
```

**Note:** Parse column shows ‚ö†Ô∏è (not ‚úÖ) because `parse_status == "FAILED"` despite 100% progress. This indicates semantic error after successful parse.

---

### Example 3: mhw4d.gms (Full Success)

**Source:** 18 logical lines total

**Metrics:**
- `error_line`: None
- `lines_parsed`: 18
- `total_lines`: 18
- `percentage`: 100.0%
- `missing_features`: None

**Dashboard Display:**
```markdown
| mhw4d | ‚úÖ | 100% (18/18) | - | ‚ö†Ô∏è | - | ‚ùå |
```

---

## Backward Compatibility

**JSON Report Schema:**
- ‚úÖ **Backward compatible** - New fields are optional
- ‚úÖ Old tooling can ignore new fields (`parse_progress_*`, `missing_features`, `error_line`)
- ‚úÖ Existing fields unchanged (`parse_status`, `parse_error`, `parse_error_type`)

**Dashboard Template:**
- ‚úÖ **Backward compatible** - New columns are additive
- ‚úÖ Old consumers can ignore new columns
- ‚úÖ Parse status still shows ‚úÖ /‚ùå for legacy tools

**Ingestion Script:**
- ‚ö†Ô∏è **Breaking change** - Command-line interface unchanged, but JSON output schema enhanced
- ‚úÖ Version field in JSON report can indicate schema version

**Recommendation:** Add schema version to JSON report:
```json
{
  "schema_version": "2.0",
  "sprint": "Sprint 8",
  ...
}
```

---

## Future Enhancements (Post-Sprint 8)

**1. True Statement-Level Counting (10+ hours)**
- Implement error-recovering parser (Lark LALR with error tokens)
- Return partial AST on errors
- Count AST nodes instead of lines
- **Accuracy:** High (exact statement counts)

**2. Advanced Missing Feature Extraction**
- Machine learning on error messages ‚Üí feature labels
- Cross-reference with GAMS documentation
- Suggest specific code changes
- **Effort:** 20+ hours

**3. Statement Dependency Graph**
- Track which statements depend on failed statements
- Show "cascade" failures
- **Effort:** 15+ hours

**4. Interactive Dashboard**
- Web-based dashboard with filters
- Click to see error details
- Progress over time (historical metrics)
- **Effort:** 40+ hours

---

## Acceptance Criteria

- [x] "Statement" defined for parse metrics (Logical lines as pragmatic approximation)
- [x] Counting mechanism designed for partial parses (Line-based counting before error)
- [x] Missing feature extraction patterns designed (Regex patterns on error messages)
- [x] Dashboard mockup created with partial metrics (New columns: Progress, Missing Features)
- [x] Ingestion pipeline updates documented (Enhanced ModelResult, new KPIs)
- [x] Backward compatibility validated (JSON schema and dashboard template)
- [x] Implementation effort estimated (6 hours total, within 4-6 hour target)

---

## References

- Sprint 7 retrospective: "Consider 'partial parse' metric"
- PROJECT_PLAN.md: Task 6 acceptance criteria
- GAMS grammar: `src/gams/gams_grammar.lark`
- Parser implementation: `src/ir/parser.py`
- Symbol definitions: `src/ir/symbols.py`
- Ingestion script: `scripts/ingest_gamslib.py`
- Current dashboard: `docs/status/GAMSLIB_CONVERSION_STATUS.md`
