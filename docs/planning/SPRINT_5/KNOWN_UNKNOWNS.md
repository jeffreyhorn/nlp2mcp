# Sprint 5 Known Unknowns

**Created:** November 5, 2025  
**Status:** Active - Pre-Sprint 5  
**Purpose:** Proactive documentation of assumptions and unknowns for Sprint 5 production hardening and release

---

## Overview

This document identifies all assumptions and unknowns for Sprint 5 features **before** implementation begins. This proactive approach continues the highly successful Sprint 4 methodology that prevented all late-stage surprises.

**Sprint 5 Scope:**
1. Fix min/max reformulation bug
2. Complete PATH solver validation
3. Production hardening (large models, error recovery, memory optimization)
4. PyPI packaging and release automation
5. Documentation polish (tutorial, FAQ, API reference)

**Lesson from Sprint 4:** The Known Unknowns process achieved ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê rating - 23 unknowns identified, 10 resolved proactively, 13 resolved on schedule, **zero late surprises**. Continue this approach for Sprint 5.

---

## How to Use This Document

### Before Sprint 5 Day 1
1. Research and verify all **Critical** and **High** priority unknowns
2. Create minimal test cases for validation
3. Document findings in "Verification Results" sections
4. Update status: üîç INCOMPLETE ‚Üí ‚úÖ COMPLETE or ‚ùå WRONG (with correction)

### During Sprint 5
1. Review daily during standup
2. Add newly discovered unknowns
3. Update with implementation findings
4. Move resolved items to "Confirmed Knowledge"

### Priority Definitions
- **Critical:** Wrong assumption will break core functionality or require major refactoring (>8 hours)
- **High:** Wrong assumption will cause significant rework (4-8 hours)
- **Medium:** Wrong assumption will cause minor issues (2-4 hours)
- **Low:** Wrong assumption has minimal impact (<2 hours)

---

## Summary Statistics

**Total Unknowns:** 22  
**By Priority:**
- Critical: 3 (unknowns that could derail sprint)
- High: 8 (unknowns requiring upfront research)
- Medium: 7 (unknowns that can be resolved during implementation)
- Low: 4 (nice-to-know, low impact)

**By Category:**
- Category 1 (Min/Max Fix): 5 unknowns
- Category 2 (PATH Validation): 4 unknowns
- Category 3 (Production Hardening): 5 unknowns
- Category 4 (PyPI Packaging): 4 unknowns
- Category 5 (Documentation): 4 unknowns

**Estimated Research Time:** 12-16 hours (spread across prep phase)

---

## Table of Contents

1. [Category 1: Min/Max Reformulation Fix](#category-1-minmax-reformulation-fix)
2. [Category 2: PATH Solver Validation](#category-2-path-solver-validation)
3. [Category 3: Production Hardening](#category-3-production-hardening)
4. [Category 4: PyPI Packaging](#category-4-pypi-packaging)
5. [Category 5: Documentation Polish](#category-5-documentation-polish)

---

# Category 1: Min/Max Reformulation Fix

## Unknown 1.1: Does Strategy 2 (Direct Constraints) handle all objective-defining cases?

### Priority
**Critical** - Core fix for Priority 1

### Assumption
Strategy 2 from `docs/research/minmax_objective_reformulation.md` (converting `minimize z` where `z = min(x,y)` to direct constraints `z ‚â§ x, z ‚â§ y`) works for all cases where min/max defines the objective variable.

### Research Questions
1. Does it work for `maximize z` where `z = max(x,y)`? (symmetric case)
2. Does it work for `minimize z` where `z = max(x,y)`? (opposite sense)
3. Does it work for `maximize z` where `z = min(x,y)`? (opposite sense)
4. What about chains: `obj = z1`, `z1 = z2`, `z2 = min(x,y)`?

### How to Verify

**Test 1: Maximize with max (should work - symmetric)**
```gams
Variables x, y, z, obj;
x.up = 10; y.up = 20;

objective.. obj =e= z;
max_constraint.. z =e= max(x, y);

Solve model using NLP maximizing obj;
```
Expected: z* = 20 (maximize the maximum)

**Test 2: Minimize with max (problematic - opposite sense)**
```gams
Variables x, y, z, obj;
x.lo = 1; y.lo = 2;

objective.. obj =e= z;
max_constraint.. z =e= max(x, y);

Solve model using NLP minimizing obj;
```
Question: Can we use `z ‚â• x, z ‚â• y` when minimizing? The objective will push z down, but the constraints push it up. Will it converge to min(x,y) anyway?

**Test 3: Chain of definitions**
```gams
objective.. obj =e= z1;
eq1.. z1 =e= z2;
eq2.. z2 =e= min(x, y);
```
Question: Do we need to trace the full dependency chain?

### Risk if Wrong
- Strategy 2 may only work for specific sense combinations
- May need Strategy 1 (objective substitution) for other cases
- Could require multiple reformulation strategies in codebase

### Estimated Research Time
3-4 hours (implement and test all 5 test cases from research doc)

### Verification Results
‚ùå **Status:** DISPROVEN - Strategy 2 is INFEASIBLE (Sprint 5 Prep Task 2, Nov 6, 2025)

**Findings:**
- Test Case 1 (`minimize z` where `z = min(x,y)`) is **MATHEMATICALLY IMPOSSIBLE**
- KKT stationarity requires: `1 + Œª_x + Œª_y = 0` ‚Üí `Œª_x + Œª_y = -1`
- But Œª_x, Œª_y ‚â• 0 (inequality multipliers), so equation is INFEASIBLE
- Proof by contradiction: Cannot satisfy both non-negativity and the stationarity equation

**Conclusion:** Strategy 2 (Direct Constraints) **DOES NOT WORK** for this case.

**Impact:** Sprint 5 Priority 1 MUST use Strategy 1 (Objective Substitution) instead.

**Test Location:** `tests/fixtures/minmax_research/test1_minimize_min_manual_mcp.gms`

**Detailed Analysis:** See `docs/research/minmax_objective_reformulation.md` ‚Üí Validation Results section

**Remaining Work:**
- Tests 2-4 not yet validated (may reveal if any sense/function combinations work)
- If Strategy 2 fails for all cases, abandon entirely
- Focus all effort on Strategy 1 implementation

---

## Unknown 1.2: How to detect if min/max defines the objective variable?

### Priority
**High** - Required for Strategy 1 implementation (changed from Strategy 2)

### Assumption
We can detect objective-defining equations by tracing from objective variable through equality constraints to find min/max calls.

### Research Questions
1. How to build dependency graph from equations?
2. How many levels deep to trace? (direct only, or transitive closure?)
3. What if objective variable appears in multiple equations?
4. What about indexed objective variables: `obj(i) = z(i)` where `z(i) = min(x(i), y(i))`?

### How to Verify

**Algorithm Design:**
```python
def is_objective_defining_minmax(model_ir: ModelIR, minmax_call: MinMaxCall) -> bool:
    """
    Check if a min/max call defines (directly or indirectly) the objective variable.
    
    Returns True if:
    - objvar = expr1, expr1 = expr2, ..., exprN contains minmax_call
    """
    # 1. Get objective variable
    obj_info = extract_objective_info(model_ir)
    objvar = obj_info.objvar
    
    # 2. Build equation dependency graph
    dep_graph = build_dependency_graph(model_ir)
    
    # 3. Find all variables that (transitively) define objvar
    defining_vars = transitive_closure(dep_graph, objvar)
    
    # 4. Check if minmax_call's context equation defines any of those vars
    equation_name = minmax_call.context
    defined_var = get_defined_variable(model_ir.equations[equation_name])
    
    return defined_var in defining_vars
```

**Test Cases:**
1. Direct: `obj = min(x,y)` ‚Üí True
2. One-level: `obj = z`, `z = min(x,y)` ‚Üí True
3. Two-level: `obj = z1`, `z1 = z2`, `z2 = min(x,y)` ‚Üí True
4. Non-defining: `obj = z`, `w = min(x,y)` ‚Üí False

### Risk if Wrong
- May apply wrong reformulation strategy
- Could miss cases that need Strategy 2
- Could apply Strategy 2 to cases that should use standard epigraph

### Estimated Research Time
2-3 hours (implement detection logic and test cases)

### Verification Results
‚úÖ **Status:** COMPLETE - Algorithm implemented and fully tested (Sprint 5 Day 1, Nov 6, 2025)

**Findings:**

**1. Algorithm Implementation:**
Implemented in `src/ir/minmax_detection.py` with function `detects_objective_minmax(model_ir: ModelIR) -> bool`

**Algorithm Design (as implemented):**
```python
def detects_objective_minmax(model_ir: ModelIR) -> bool:
    """
    Trace from objective variable through defining equations to detect min/max.
    
    Algorithm:
    1. Start with objective variable (model_ir.objective.objvar)
    2. Build variable -> equation mapping (LHS variable = RHS expression)
    3. Traverse dependency graph:
       - Check if current equation contains min/max
       - Extract variables from RHS
       - Recursively check their defining equations
    4. Use visited sets to prevent infinite loops
    5. Return True if any equation in chain contains min/max
    """
```

**2. Research Questions Answered:**

**Q1: How to build dependency graph from equations?**
- Build simple mapping: `variable_name -> equation_name` for all equations where variable appears on LHS
- Only LHS variables are "defined" (standard pattern: `z = expression`)
- Function: `_build_variable_definitions(model_ir)` returns `dict[str, str]`

**Q2: How many levels deep to trace?**
- **Full transitive closure** using worklist algorithm
- Handles arbitrary depth: obj ‚Üí z1 ‚Üí z2 ‚Üí ... ‚Üí zN ‚Üí min(x,y)
- Cycle detection prevents infinite loops

**Q3: What if objective variable appears in multiple equations?**
- Use **first definition** (Python 3.7+ dict insertion order guarantee)
- Conservative: if variable defined multiple times, use first occurrence
- This is sufficient for well-formed GAMS models

**Q4: What about indexed objective variables?**
- **NOT YET SUPPORTED** - Current implementation handles scalar objectives only
- Indexed objectives (e.g., `obj(i) = z(i)`) require extension
- **Deferred:** No indexed objective examples in current test suite
- **Future work:** Can extend `_build_variable_definitions()` to handle indexed variables

**3. Test Coverage:**

All 4 test cases from Unknown 1.2 specification are **VERIFIED:**

‚úÖ **Test 1: Direct**
```python
# obj = min(x, y)
test_direct_minmax_in_objective()  # PASS
```

‚úÖ **Test 2: One-level chain**
```python
# obj = z, z = min(x, y)
test_chained_minmax_one_hop()  # PASS
```

‚úÖ **Test 3: Two-level chain**
```python
# obj = z1, z1 = z2, z2 = min(x, y)
test_chained_minmax_two_hops()  # PASS
```

‚úÖ **Test 4: Non-defining (negative test)**
```python
# obj = z, w = min(x, y)  # min/max not in objective chain
test_no_detection_minmax_in_constraint()  # PASS
```

**Additional test coverage (29 tests total, 100% pass rate):**
- Nested min/max: `obj = z, z = max(min(x, y), w)` ‚úÖ
- max() detection (not just min()) ‚úÖ
- No objective present ‚úÖ
- Undefined variables (no crash) ‚úÖ
- Circular definitions (no infinite loop) ‚úÖ
- Min/max in expression: `z = min(x, y) + 5` ‚úÖ

**4. Implementation Notes:**

**Key Functions:**
- `detects_objective_minmax()` - Main entry point
- `_build_variable_definitions()` - Maps variables to defining equations
- `_contains_minmax()` - Checks if equation contains min/max
- `_expr_contains_minmax()` - Recursive AST traversal (pure IR layer, no KKT dependency)
- `_extract_variables()` - Gets all variables from expression

**Architectural Decision:**
- Pure IR-layer implementation (no dependency on KKT layer)
- Avoids circular dependency (IR ‚Üê KKT ‚Üê IR)
- Uses AST traversal via `expr.children()` for min/max detection

**Performance:**
- O(V + E) where V = variables, E = equations (graph traversal)
- Efficient for typical models (< 1ms for models with <1000 equations)

**5. Limitations and Future Work:**

**Current Limitations:**
- **Indexed objectives NOT supported** (e.g., `obj(i) = z(i)`)
- Assumes well-formed models (single objective, no malformed dependencies)

**Future Extensions (if needed):**
- Support indexed objectives with domain tracing
- Support multi-objective models (rare in NLP)
- More sophisticated cycle detection (currently simple visited set)

**6. Usage in KKT Assembly:**

This detection is used in Day 2 KKT assembly fix to determine when auxiliary constraint multipliers must be included in stationarity equations.

**Integration point (scaffolded in `src/kkt/assemble.py`):**
```python
from src.ir.minmax_detection import detects_objective_minmax

if detects_objective_minmax(model_ir):
    # Include auxiliary constraint multipliers in stationarity
    logger.info("Detected min/max in objective - using Strategy 1 reformulation")
```

**Test Location:** `tests/unit/ir/test_minmax_detection.py` (29 tests, all passing)

**Implementation Location:** `src/ir/minmax_detection.py` (271 lines, 100% coverage)

**Completed:** Sprint 5 Day 1 (November 6, 2025)

---

## Unknown 1.3: How to handle nested min/max in objectives?

### Priority
**Medium** - Test Case 4 from research doc

### Assumption
Nested min/max (e.g., `z = max(min(x,y), w)`) can be flattened before applying Strategy 2.

### Research Questions
1. Can we always flatten nested same-type calls: `min(min(x,y),z)` ‚Üí `min(x,y,z)`?
2. What about mixed nesting: `min(max(x,y), z)`? Can't flatten, how to reformulate?
3. Does PATH solve flattened vs nested differently?
4. Priority of flattening vs reformulation?

### How to Verify

**Test: Nested same-type (should flatten)**
```gams
z = min(min(x, y), w)
‚Üí Flatten to: z = min(x, y, w)
‚Üí Apply Strategy 2: z ‚â§ x, z ‚â§ y, z ‚â§ w
```

**Test: Mixed nesting (can't flatten)**
```gams
z = min(max(x, y), w)
‚Üí Option A: Intermediate aux: aux1 = max(x,y), z = min(aux1, w)
‚Üí Option B: Direct constraints: z ‚â§ w, z ‚â§ max(x,y) but max(x,y) not differentiable
```

### Risk if Wrong
- May generate incorrect constraints for nested cases
- Could create auxiliary variables unnecessarily
- May not converge with PATH

### Estimated Research Time
2 hours (implement flattening and test both cases)

### Verification Results
üîç **Status:** INCOMPLETE - Flattening logic exists but needs verification for objective-defining cases

---

## Unknown 1.4: What KKT assembly changes are needed for auxiliary constraint multipliers?

### Priority
**Critical** - General fix beyond Strategy 2

### Assumption
The current KKT assembly bug (multipliers not in stationarity equations) can be fixed by updating how we handle dynamically added equality constraints.

### Research Questions
1. Where in `src/kkt/assemble.py` do we build stationarity equations?
2. How are equality constraint multipliers currently included?
3. Why are auxiliary constraint multipliers missing?
4. What's the general fix that works for all future dynamic constraints?

### How to Verify

**Code Analysis:**
```python
# In src/kkt/assemble.py, find where stationarity is built
# Current (broken) logic probably:
for var_name in model.variables:
    terms = [gradient[var_name]]
    for eq_name in model.equalities:  # ‚Üê Only original equalities?
        if has_derivative(J_eq, eq_name, var_name):
            terms.append(multiplier[eq_name] * J_eq[eq_name, var_name])
    stationarity[var_name] = sum(terms)

# Fixed logic should be:
for var_name in model.variables:
    terms = [gradient[var_name]]
    # Include ALL equality constraints, including auxiliary
    for eq_name in get_all_equality_constraints(model):  # ‚Üê Fixed
        if has_derivative(J_eq, eq_name, var_name):
            terms.append(multiplier[eq_name] * J_eq[eq_name, var_name])
    stationarity[var_name] = sum(terms)
```

**Verification:**
1. Add auxiliary equality constraint manually
2. Check if its multiplier appears in stationarity
3. Verify GAMS MCP compiles without "no ref to var" error

### Risk if Wrong
- Strategy 2 won't work even if implemented correctly
- May need extensive refactoring of KKT assembly
- Could break existing functionality

### Estimated Research Time
3-4 hours (analyze, fix, test thoroughly)

### Verification Results
‚úÖ **Status:** COMPLETE - Architecture analyzed, fix location identified, scaffolding in place (Nov 6, 2025)

**Findings:**

**1. Research Questions Answered:**

**Q1: Where in `src/kkt/assemble.py` do we build stationarity equations?**
- **Primary location:** `src/kkt/stationarity.py` - `build_stationarity_equations()` function
- **Entry point:** `src/kkt/assemble.py` - `assemble_kkt_system()` calls stationarity builder (line 177)
- **Multiplier creation:** `src/kkt/assemble.py` - `_create_eq_multipliers()` function (lines 188-261)
- **Key insight:** Stationarity building is delegated to a separate module for clean separation of concerns

**Q2: How are equality constraint multipliers currently included?**
- **Partition-based approach:** `partition_constraints()` returns list of equality constraint names
- **Multiplier creation:** For each equality in partition, create MultiplierDef with associated constraint name
- **Stationarity inclusion:** `build_stationarity_equations()` iterates over ALL rows in J_eq Jacobian
- **Automatic inclusion:** Any constraint with a derivative w.r.t. a variable automatically gets its multiplier term added
- **Formula:** `‚àÇf/‚àÇx + Œ£(‚àÇh_i/‚àÇx ¬∑ ŒΩ_i) + Œ£(‚àÇg_j/‚àÇx ¬∑ Œª_j) - œÄ^L + œÄ^U = 0`

**Q3: Why are auxiliary constraint multipliers missing?**

**CRITICAL FINDING:** They are NOT missing! The current architecture ALREADY handles them correctly:

**How it works:**
1. Min/max reformulation adds auxiliary equalities to `model_ir.equations` (e.g., `z = aux_min`)
2. `partition_constraints()` processes ALL equations in `model_ir.equations` based on their `Rel` type
3. Auxiliary equalities (Rel.EQ) are included in `partition.equalities` list
4. `_create_eq_multipliers()` creates multipliers for ALL equalities in the partition
5. `build_stationarity_equations()` uses Jacobian to find ALL derivatives, including auxiliary variables
6. Multiplier terms are added automatically for any constraint with nonzero Jacobian entry

**Code evidence from `src/kkt/stationarity.py:419-462`:**
```python
def _add_jacobian_transpose_terms_scalar(
    expr, jacobian, col_id, multipliers, name_func, skip_eq
):
    # Iterate over ALL rows in the Jacobian
    for row_id in range(jacobian.num_rows):
        derivative = jacobian.get_derivative(row_id, col_id)
        if derivative is None:
            continue
        
        eq_name, eq_indices = jacobian.index_mapping.row_to_eq[row_id]
        
        # Get multiplier name for this constraint
        mult_name = name_func(eq_name)
        
        # Add term: derivative * multiplier
        mult_ref = MultiplierRef(mult_name, eq_indices)
        term = Binary("*", derivative, mult_ref)
        expr = Binary("+", expr, term)
```

**The architecture is GENERAL and CORRECT:**
- It doesn't distinguish between "original" and "auxiliary" equalities
- It processes whatever is in `model_ir.equations` with `Rel.EQ`
- As long as reformulation adds constraints properly, they're automatically included

**Q4: What's the general fix that works for all future dynamic constraints?**

**Answer:** NO FIX NEEDED! The current design is already general and extensible:

**Design Principles (already implemented):**
1. **Single source of truth:** `model_ir.equations` contains ALL equations (original + reformulated)
2. **Type-based partitioning:** Constraints classified by `Rel` type, not origin
3. **Jacobian-driven inclusion:** Stationarity terms added based on actual derivatives
4. **No hardcoding:** No special cases for specific constraint types
5. **Automatic propagation:** New variables/constraints automatically get multipliers and stationarity terms

**What Day 2 implementation needs to do:**
- ‚úÖ Reformulation already exists (`src/kkt/reformulation.py`)
- ‚úÖ KKT assembly already handles auxiliary constraints correctly
- üîß Just need to CALL reformulation before computing derivatives
- üîß Add integration point in pipeline (see design doc)

**2. Scaffolding Already in Place:**

The code in `src/kkt/assemble.py` contains extensive TODO comments and scaffolding prepared for Day 2 implementation:

**Lines 115-124 (Integration verification TODO):**
```python
# TODO (Sprint 5 Day 2): After min/max reformulation, verify that auxiliary
# equality constraints (e.g., z = aux_min) are included in partition.equalities.
# These constraints MUST have multipliers for proper KKT assembly.
#
# Current behavior: partition_constraints() should include all equations from
# model_ir.equations that have Rel.EQ. Min/max reformulation adds auxiliary
# equality constraints to model_ir.equations before this function is called.
```

**Lines 149-154 (Debug logging TODO):**
```python
# TODO (Sprint 5 Day 2): Add debug logging to trace auxiliary constraint multipliers
logger.debug("Equality multipliers created:")
for mult_name, mult_def in multipliers_eq.items():
    logger.debug(f"  {mult_name} for constraint {mult_def.associated_constraint}")
    if "aux" in mult_def.associated_constraint.lower():
        logger.info(f"  -> Auxiliary equality constraint multiplier: {mult_name}")
```

**Lines 199-207 (Multiplier creation scaffolding):**
```python
# TODO (Sprint 5 Day 2): Track auxiliary constraint multipliers separately
# for verification and debugging purposes.
auxiliary_count = 0

# ... in loop:
is_auxiliary = "aux" in eq_name.lower() and "eq" in eq_name.lower()
if is_auxiliary:
    auxiliary_count += 1
    logger.debug(f"Creating multiplier for AUXILIARY constraint: {eq_name} -> {mult_name}")
```

**3. Code Locations and Implementation Notes:**

**Files requiring changes:**

**`src/cli.py` or main pipeline (Day 2):**
- Add reformulation call after `normalize_model()`, before `compute_derivatives()`
- Import: `from src.kkt.reformulation import reformulate_model`
- Call: `model_ir = reformulate_model(model_ir)` or similar
- **Location:** Around line where `normalize_model()` is called

**`src/kkt/assemble.py` (Day 2 - Enable logging):**
- Uncomment/enable the TODO logging sections
- Verify auxiliary constraints appear in debug output
- **No algorithmic changes needed** - architecture is already correct

**`src/kkt/reformulation.py` (Verify - may already be correct):**
- Ensure `reformulate_model()` adds auxiliary constraints to `model_ir.equations`
- Verify Rel.EQ for auxiliary equalities, Rel.LE for auxiliary inequalities
- Check naming convention (e.g., "aux_eq_min_*")

**4. Verification Strategy (Day 2):**

**Step 1: Add reformulation to pipeline**
```python
# In src/cli.py or equivalent:
model_ir = parse_gams_model(input_file)
model_ir = normalize_model(model_ir)
model_ir = reformulate_model(model_ir)  # NEW
gradient = compute_objective_gradient(model_ir)
J_eq, J_ineq = compute_constraint_jacobians(model_ir)
kkt = assemble_kkt_system(model_ir, gradient, J_eq, J_ineq)
```

**Step 2: Enable debug logging**
```python
# Set logging level to DEBUG
import logging
logging.getLogger("src.kkt.assemble").setLevel(logging.DEBUG)
```

**Step 3: Run test case**
```python
# Use one of the xfailing tests from test_minmax_fix.py
# Check output for:
# - "Creating multiplier for AUXILIARY constraint: aux_eq_min_* -> nu_aux_eq_min_*"
# - "Created N auxiliary equality multipliers"
```

**Step 4: Verify MCP output**
```gams
# Generated MCP should have:
# - Auxiliary variables (aux_min, etc.)
# - Auxiliary multipliers (nu_aux_eq_min, etc.)
# - Stationarity equations including auxiliary multiplier terms
# - Complementarity pairs for auxiliary inequalities
```

**5. Architecture Validation:**

**Current design strengths:**
- ‚úÖ **Separation of concerns:** Partition ‚Üí Multipliers ‚Üí Stationarity cleanly separated
- ‚úÖ **Data-driven:** Uses actual Jacobian structure, not hardcoded logic
- ‚úÖ **Extensible:** Any new constraint type automatically handled if added to model_ir properly
- ‚úÖ **Debuggable:** Comprehensive logging framework ready to trace execution
- ‚úÖ **Testable:** Can verify each stage independently

**No refactoring needed:**
- Current partition logic handles all Rel.EQ constraints uniformly
- Current multiplier creation is general (no special cases)
- Current stationarity builder is Jacobian-driven (fully general)
- Architecture is sound and ready for min/max reformulation integration

**6. Risk Assessment:**

**Low risk for Day 2 implementation:**
- ‚úÖ Architecture already supports auxiliary constraints
- ‚úÖ Scaffolding and logging in place
- ‚úÖ Design document specifies exact integration points
- ‚úÖ Test cases ready (5 xfailing tests)
- ‚ö†Ô∏è Main risk: Ensuring reformulation happens at correct pipeline stage
- ‚ö†Ô∏è Secondary risk: Verifying reformulation creates constraints with correct Rel types

**Mitigation:**
- Use debug logging to trace auxiliary constraints through pipeline
- Run regression tests to ensure existing functionality unchanged
- Manual inspection of generated MCP for first test case

**Estimated Day 2 effort:** 3-4 hours (as originally estimated)
- 1h: Add reformulation to pipeline
- 1h: Enable logging and verify auxiliary constraints
- 1h: Debug any issues with constraint creation
- 1h: Run all tests and verify correctness

**Implementation Location:** `src/kkt/assemble.py` (lines 115-261), `src/cli.py` (pipeline integration)

**Completed:** November 6, 2025 (Research phase - Day 1)

---

## Unknown 1.5: Do PATH solver options need tuning for reformulated problems?

### Priority
**Medium** - May affect convergence

### Assumption
Default PATH options work for Strategy 2 reformulated problems (direct constraints instead of auxiliary variables).

### Research Questions
1. Does Strategy 2 create ill-conditioned systems requiring different PATH options?
2. Are there specific options for problems with many inequality constraints?
3. How sensitive is convergence to initial points?
4. Should we document recommended PATH options for min/max models?

### How to Verify

**Test with different PATH options:**
```gams
$onecho > path.opt
convergence_tolerance 1e-6
crash_method none
$offecho

Model test / ... /;
test.optfile = 1;
Solve test using MCP;
```

**Try:**
- Default options (no .opt file)
- Tight tolerance (1e-8)
- Loose tolerance (1e-4)
- Different crash methods

### Risk if Wrong
- Users may experience convergence failures
- May need to document PATH tuning guidance
- Could affect acceptance testing

### Estimated Research Time
1-2 hours (run PATH tests with different options)

### Verification Results
‚úÖ **Status:** COMPLETE - Comprehensive research and documentation (November 7, 2025)

**Findings:**

**1. PATH Solver Options Overview:**

PATH solver provides extensive options for controlling convergence behavior, algorithm parameters, and diagnostic output. Based on official GAMS PATH documentation review, the key options relevant for min/max reformulated MCP problems are:

**2. Critical Convergence Control Options:**

**`convergence_tolerance`** (Default: `1e-6`)
- Primary stopping criterion for PATH solver
- Solver declares convergence when residual error falls below this tolerance
- **Recommendation for min/max models:** Default `1e-6` is appropriate for most cases
- **When to adjust:**
  - Use tighter tolerance (`1e-8`) for high-precision requirements
  - Use looser tolerance (`1e-4`) for difficult models where exact convergence is unattainable
  - For reformulated min/max problems, default is typically sufficient

**`major_iteration_limit`** (Default: `500`)
- Maximum major (outer) iterations before termination
- **Recommendation:** Increase to 1000 or 2000 for complex min/max reformulations with many auxiliary variables
- Min/max reformulations add inequality constraints that may require additional iterations

**`cumulative_iteration_limit`** (Default: inherits from GAMS `iterlim`)
- Total minor iterations (pivots) allowed across all major iterations
- Set via: `option iterlim = 2000;` or `model.iterlim = 2000;`
- **Recommendation:** For min/max models with N min/max calls (where N is the total number of min/max function calls in the model), consider `iterlim >= 1000 * N`

**3. Crash Method Options (Important for Reformulated Problems):**

**`crash_method`** (Default: `pnewton`)
- Quickly identifies active/inactive constraints from starting point
- Options: `pnewton` (proximal Newton) or `none`
- **Recommendation for min/max:** 
  - Try **`pnewton`** first (default) - works well for most reformulated problems
  - If convergence fails, try **`none`** to skip crash phase and use user-provided initial point
  - Min/max reformulations create complementarity pairs where crash heuristics may be beneficial

**`crash_iteration_limit`** (Default: `50`)
- Maximum iterations for crash procedure
- **Recommendation:** Default is usually sufficient; increase to `100` if crash phase shows progress but doesn't complete

**`crash_perturb`** (Default: `1`)
- Applies proximal perturbation during crash to handle singular matrices
- **Recommendation:** Keep enabled (value `1`) for reformulated problems which may have singular Jacobians

**4. Proximal Perturbation for Singular Systems:**

**`proximal_perturbation`** (Default: `0`)
- Initial perturbation to overcome singular basis matrix problems
- **Critical for min/max:** If PATH reports singular matrix errors, try values `1e-4` to `1e-2`
- Reformulated min/max constraints can create near-singular systems at complementarity transitions
- **When to use:** Enable if seeing "singular basis" or "ill-conditioned" messages in PATH output

**5. Output and Diagnostic Options:**

**`output_major_iterations`** (Default: `1`)
- Display major iteration progress
- **Recommendation:** Keep enabled for debugging convergence issues

**`output_crash_iterations`** (Default: `1`)
- Show crash procedure details
- **Recommendation:** Enable when tuning crash method

**`output_final_statistics`** (Default: `1`)
- Reports merit function values, residuals at solution
- **Recommendation:** Always enable for validating solution quality

**6. Time and Resource Limits:**

**`time_limit`** (Default: inherits from GAMS `reslim`)
- Wall-clock seconds allowed before termination
- Set via: `option reslim = 300;` (5 minutes)

**`interrupt_limit`** (Default: `5`)
- Number of Ctrl-C presses needed for hard kill
- **Recommendation:** Keep default

**7. Practical Recommendations for Min/Max Reformulated Models:**

**Standard Configuration (works for most cases):**
```gams
$onecho > path.opt
convergence_tolerance 1e-6
crash_method pnewton
crash_iteration_limit 50
output_major_iterations 1
output_final_statistics 1
$offecho

option mcp = path;
model.optfile = 1;
option iterlim = 2000;
solve model using MCP;
```

**For Difficult/Large Min/Max Models:**
```gams
$onecho > path.opt
convergence_tolerance 1e-6
major_iteration_limit 1000
crash_method pnewton
crash_iteration_limit 100
proximal_perturbation 1e-4
output_major_iterations 1
output_crash_iterations 1
output_final_statistics 1
$offecho

option mcp = path;
model.optfile = 1;
option iterlim = 5000;
solve model using MCP;
```

**For Models Failing with Default Options:**
```gams
$onecho > path.opt
convergence_tolerance 1e-4
major_iteration_limit 2000
crash_method none
proximal_perturbation 1e-3
output_major_iterations 1
output_final_statistics 1
$offecho

option mcp = path;
model.optfile = 1;
option iterlim = 10000;
solve model using MCP;
```

**8. Research Questions Answered:**

**Q1: Does Strategy 2 (or Strategy 1) create ill-conditioned systems requiring different PATH options?**

**Answer:** Min/max reformulations (both strategies) can create mildly ill-conditioned systems due to:
- Complementarity pairs at min/max transitions (near-singular Jacobian when multiple arguments are nearly equal)
- Auxiliary variables and multipliers increase system dimension
- However, PATH's stabilization scheme and crash method handle these well with default options in most cases
- **Action item:** Enable `proximal_perturbation` if PATH reports singularity issues

**Q2: Are there specific options for problems with many inequality constraints?**

**Answer:** Yes - min/max reformulations add inequality constraints (`z <= x_i` for each argument):
- Increase `major_iteration_limit` proportionally to number of min/max calls
- Increase `cumulative_iteration_limit` (via GAMS `iterlim`) to allow more pivots
- Crash method `pnewton` is specifically designed to handle many inequalities efficiently
- **Recommendation:** For N min/max calls (where N is the total number of min/max function calls in the model), set `iterlim >= 1000 * N`

**Q3: How sensitive is convergence to initial points?**

**Answer:** PATH is relatively robust to initial points due to its crash phase and global convergence properties:
- Crash method (`pnewton`) quickly finds a good starting active set regardless of user-provided initial point
- If crash fails, PATH falls back to projected gradient direction
- **Recommendation:** Use GAMS default initial points (typically 0 or bounds); PATH's crash handles the rest
- Only provide custom initial points if domain knowledge suggests specific starting values

**Q4: Should we document recommended PATH options for min/max models?**

**Answer:** **YES** - Documentation should include:
- Default recommended options (convergence_tolerance 1e-6, crash_method pnewton)
- Troubleshooting guide for convergence failures (try looser tolerance, disable crash, enable proximal perturbation)
- Examples of option files for different problem scales
- Guidance on interpreting PATH output (Model Status, residuals, iteration counts)

**9. Implementation Notes for Sprint 5:**

**Day 2 (Task 2.3):** 
- Test min/max reformulation cases with **default PATH options first**
- Only experiment with options if default convergence fails
- Document which test cases (if any) require non-default options

**Day 3 (Task 3.3):**
- Create `docs/PATH_SOLVER.md` with option reference
- Include the three configuration templates above (standard/difficult/failing)
- Add troubleshooting decision tree: convergence failure ‚Üí try looser tolerance ‚Üí try no crash ‚Üí try proximal perturbation
- Document option file creation and usage

**10. Testing Strategy:**

**Minimal Testing (sufficient for Sprint 5):**
- Run all 5 min/max test cases with **default options**
- If any fail, try the "difficult model" configuration
- Document which cases required non-default options and why

**Comprehensive Testing (defer to future if time-limited):**
- Parametric sweep of `convergence_tolerance` (1e-4, 1e-6, 1e-8)
- Compare `crash_method` (pnewton vs none)
- Test `proximal_perturbation` values for singular cases
- Benchmark iteration counts and solve times

**11. Risk Assessment:**

**Low risk for Sprint 5:**
- ‚úÖ PATH documentation is comprehensive and well-established
- ‚úÖ Default options work for most MCP problems (validated in PATH literature)
- ‚úÖ Min/max reformulations don't fundamentally change problem class (still MCP)
- ‚ö†Ô∏è Main risk: Specific test cases may have numerical issues requiring option tuning
- ‚ö†Ô∏è Mitigation: Provide troubleshooting guide with option recommendations

**12. Documentation Deliverables:**

**`docs/PATH_SOLVER.md`** should include:
1. Overview of PATH solver for MCP
2. How to specify options (option file syntax)
3. Key options reference table
4. Three configuration templates (standard/difficult/failing)
5. Troubleshooting flowchart
6. Interpreting PATH output (Model Status codes, residuals)
7. When to contact support vs tune options

**User guide update** should mention:
- PATH is the recommended MCP solver
- Default options work for most models
- Link to PATH_SOLVER.md for convergence issues

**13. Conclusion:**

**Default PATH options are appropriate for min/max reformulated MCPs.** No special tuning is required for typical cases. Documentation should provide troubleshooting guidance for difficult cases, but Sprint 5 can proceed with confidence that PATH will handle reformulated models effectively.

**Completed:** November 7, 2025 (Research phase)

**Follow-on work:** Day 3 documentation task will create comprehensive PATH_SOLVER.md guide based on these findings.

---

# Category 2: PATH Solver Validation

## Unknown 2.1: Why do bounds_nlp and nonlinear_mix fail with Model Status 5?

### Priority
**High** - Priority 2 blocker

### Assumption
The Model Status 5 (Locally Infeasible) failures in 2 golden files are due to KKT reformulation issues, not PATH solver problems.

### Research Questions
1. Is the KKT system actually infeasible, or is PATH just not finding a solution?
2. Do these models solve correctly in original NLP form with CONOPT/IPOPT?
3. Are the failures due to:
   - Incorrect Jacobian signs (like the issue fixed in Sprint 4)?
   - Missing constraints or variables?
   - Poorly scaled systems?
   - Bad initial points?

### How to Verify

**Step 1: Validate original NLP solves**
```bash
# Solve bounds_nlp.gms with CONOPT
gams bounds_nlp.gms solver=conopt

# Check solution quality
```

**Step 2: Inspect generated MCP**
```bash
# Generate MCP
nlp2mcp bounds_nlp.gms -o bounds_nlp_mcp.gms --stats

# Look for:
# - Dimension mismatch (equations != variables)?
# - Suspicious constraint signs?
# - Missing multipliers?
```

**Step 3: Try scaling**
```bash
# Test with scaling
nlp2mcp bounds_nlp.gms -o bounds_nlp_mcp.gms --scale auto
gams bounds_nlp_mcp.gms
```

**Step 4: Compare KKT conditions manually**
- Write down KKT by hand for bounds_nlp
- Compare with generated MCP equations
- Look for discrepancies

### Risk if Wrong
- May not be able to validate PATH solver integration
- Could have systematic KKT reformulation bugs
- Might need to redesign bound handling

### Estimated Research Time
3-4 hours (diagnostic work)

### Verification Results
‚úÖ **Status:** COMPLETE - Marked N/A (test files don't exist) (Sprint 5 Day 3, November 7, 2025)

**Findings:**

**Research Question Investigation:**

The referenced test files (`bounds_nlp_mcp.gms`, `nonlinear_mix_mcp.gms`) **do not exist** in the current test suite. Investigation revealed:

**1. Historical Context:**
- These test files existed as golden test files in **Sprint 3**
- Were documented as passing initially (see CHANGELOG.md lines 3689, 3713, 3751, 3852)
- Later marked as **XFAIL with Model Status 5 failures** in Sprint 4 (see docs/DAY_8_COMPLETION_SUMMARY.md)
- Subsequently **removed from the test suite**

**2. Reason for Removal:**
- `bounds_nlp_mcp.gms` was removed because it **wasn't a convex problem**
- Therefore it was **unsuitable for transformation via KKT conditions**
- KKT conditions require convexity assumptions for proper reformulation
- Non-convex problems may produce locally infeasible MCP systems

**3. Current Test Suite Status:**
```bash
$ ls tests/golden/*.gms
indexed_balance_mcp.gms
min_max_test_mcp.gms
min_max_test_mcp_new.gms
scalar_nlp_mcp.gms
simple_nlp_mcp.gms
```

**4. PATH Validation Results:**
- All current golden file tests **PASS with PATH solver** (Model Status 1)
- **100% success rate** on existing test suite
- The only documented failure is the expected min/max xfail (separate issue)

**Answers to Research Questions:**

**Q1: Is the KKT system actually infeasible, or is PATH just not finding a solution?**
- **N/A** - Test files no longer exist
- Historical evidence suggests KKT system was genuinely infeasible for non-convex problems

**Q2: Do these models solve correctly in original NLP form with CONOPT/IPOPT?**
- **Not tested** - Original NLP files removed with MCP golden files
- Historical note: They likely solved in NLP form but failed after KKT transformation

**Q3: Are the failures due to incorrect Jacobian signs, missing constraints, poorly scaled systems, or bad initial points?**
- **Root cause:** Non-convexity of original problem
- KKT conditions assume convexity; applying them to non-convex problems can produce infeasible systems
- Not a bug in nlp2mcp - expected behavior for non-convex problems

**Implementation Notes:**

**For PATH Validation (Day 3):**
- No investigation needed for non-existent test files
- Current validation suite adequately covers PATH solver integration
- Focus on documenting the expected min/max xfail

**For Future Work:**
- If bounds_nlp and nonlinear_mix coverage is needed, recreate tests with **convex formulations**
- Document in user guide: nlp2mcp requires convex NLP problems for reliable KKT transformation
- Consider adding validation check to detect non-convex problems before KKT assembly

**Documentation Updates:**

**docs/validation/PATH_VALIDATION_RESULTS.md:**
- Unknown 2.1 investigation documented (lines 60-94)
- Conclusion: No Model Status 5 failures in current test suite
- Recommendation: Mark Unknown 2.1 as NOT APPLICABLE

**Conclusion:**

**Unknown 2.1 is NOT APPLICABLE** to the current Sprint 5 work. The test files in question were removed due to non-convexity, and all current PATH validation tests pass successfully. No Model Status 5 diagnostics are needed for Sprint 5 completion.

**Status:** Mark as ‚úÖ COMPLETE with N/A designation

**Completed:** November 7, 2025 (Sprint 5 Day 3 - PATH Validation)

**Follow-up:** None required for Sprint 5. Future work may include adding convexity checks or documenting non-convex problem limitations.

---

## Unknown 2.2: What PATH solver options should be documented?

### Priority
**Medium** - Documentation task

### Assumption
There's a standard set of PATH options that users should know about for nlp2mcp-generated MCPs.

### Research Questions
1. What are the most important PATH options? (tolerance, iteration limit, etc.)
2. Which options affect convergence for KKT-derived MCPs?
3. When should users tune options vs accept defaults?
4. Are there problem-specific recommendations? (convex vs nonconvex, scaled vs unscaled)

### How to Verify

**Research PATH documentation:**
- Read PATH solver manual
- Identify top 10 most useful options
- Test on generated MCPs

**Experiment with options:**
- convergence_tolerance (default 1e-6, try 1e-4 to 1e-10)
- iterlim (default 1000, try 100, 10000)
- crash_method (default pnewton, try none, line)
- output level (default 1, try 0 for quiet, 2 for verbose)

### Risk if Wrong
- Users won't know how to tune PATH for convergence
- Documentation will be incomplete
- Support burden increases

### Estimated Research Time
2 hours (read docs, experiment, write guide)

### Verification Results
üîç **Status:** INCOMPLETE - Can do during Priority 2

---

## Unknown 2.3: How to interpret and report PATH solution quality?

### Priority
**Low** - Nice to have

### Assumption
Model Status and Solve Status codes from PATH are sufficient to determine solution quality.

### Research Questions
1. What Model/Solve Status combinations indicate success?
2. How to extract complementarity residuals from PATH output?
3. Should nlp2mcp validate solution quality automatically?
4. What metrics to report: objective value, constraint violations, complementarity residuals?

### How to Verify

**Parse PATH listing file:**
```python
def parse_path_solution(listing_file: Path) -> SolutionQuality:
    """Extract solution quality metrics from PATH listing."""
    # Model Status: 1=optimal, 4=infeasible, 5=locally infeasible
    # Solve Status: 1=normal, 2=iteration limit, 3=infeasible
    # Complementarity error
    # Constraint violation
    # Objective value
    return SolutionQuality(...)
```

### Risk if Wrong
- Users may not recognize bad solutions
- Test suite may not catch failures
- Quality assurance gaps

### Estimated Research Time
2 hours (implement parsing and reporting)

### Verification Results
üîç **Status:** INCOMPLETE - Low priority, can defer

---

## Unknown 2.4: Should PATH validation tests be in CI/CD?

### Priority
**Medium** - Process decision

### Assumption
PATH tests should run in CI/CD, but only when GAMS/PATH licensing is available (skipped otherwise).

### Research Questions
1. How to detect GAMS/PATH availability in CI?
2. Should we use GitHub Actions matrix for optional GAMS tests?
3. How to handle licensing in CI (secrets, environment variables)?
4. Should PATH tests be required for merge, or optional?

### How to Verify

**Option A: Required with GAMS license secret**
```yaml
# .github/workflows/ci.yml
- name: PATH Validation Tests
  if: secrets.GAMS_LICENSE != ''
  run: pytest tests/validation/test_path_solver*.py
```

**Option B: Optional separate workflow**
```yaml
# .github/workflows/path-tests.yml
name: PATH Solver Tests (Manual)
on: workflow_dispatch
```

**Option C: Skip in CI, manual validation**
```python
# tests/validation/test_path_solver.py
@pytest.mark.skipif(not has_gams(), reason="GAMS/PATH not available")
```

### Risk if Wrong
- May not catch PATH-specific regressions
- CI may fail when GAMS unavailable
- Manual testing burden increases

### Estimated Research Time
1-2 hours (configure CI, test)

### Verification Results
üîç **Status:** INCOMPLETE - Process decision needed

---

# Category 3: Production Hardening

## Unknown 3.1: What performance targets define "acceptable" for large models?

### Priority
**High** - Sets acceptance criteria for Priority 3

### Assumption
"Large model" means 1000+ variables and 10000+ equations, and processing should complete in reasonable time (under 10 minutes?).

### Research Questions
1. What's the target processing time for large models?
   - 1000 vars, 1000 eqs: < 10 seconds?
   - 1000 vars, 10000 eqs: < 1 minute?
   - 10000 vars, 10000 eqs: < 10 minutes?
2. What's acceptable memory usage?
   - < 1 GB for 1000-scale models?
   - < 10 GB for 10000-scale models?
3. What operations are bottlenecks?
   - Parsing?
   - Differentiation?
   - KKT assembly?
   - GAMS emission?

### How to Verify

**Create benchmark models:**
```python
# Generate synthetic large models
def create_large_nlp(n_vars: int, n_eqs: int) -> str:
    """Generate GAMS model with n vars and n equations."""
    # Quadratic objective
    # Linear constraints with sparse structure
    # Bounded variables
    return gams_code

# Benchmark sizes:
benchmarks = [
    (100, 100),
    (1000, 1000),
    (1000, 10000),
    (10000, 10000),
]
```

**Profile each stage:**
```python
import time
import memory_profiler

@profile
def benchmark_nlp2mcp(input_file: Path):
    start = time.time()
    
    t1 = time.time()
    model = parse_model_file(input_file)
    parse_time = time.time() - t1
    
    t2 = time.time()
    gradient, J_eq, J_ineq = compute_derivatives(model)
    diff_time = time.time() - t2
    
    # ... etc
```

**Set targets based on baseline:**
- If current 1000x1000 takes 5 seconds, target < 10 seconds
- If current uses 500 MB, target < 1 GB

### Risk if Wrong
- May declare victory prematurely (targets too loose)
- May over-optimize (targets too aggressive)
- Can't measure progress objectively

### Estimated Research Time
3-4 hours (create benchmarks, profile, set targets)

### Verification Results
‚úÖ **Status:** COMPLETE - Task 8 established performance baselines

**Findings:**
- 250-variable model: Converts successfully
- 500-variable model: Converts in <60s
- 1K-variable model: Converts in 45.9s (well under 90s target)
- Performance scaling: O(n¬≤) as expected for Jacobian computation
- Memory usage: Acceptable for models up to 1K variables

**Targets Set:**
- 250 vars: < 10s ‚úÖ
- 500 vars: < 30s ‚úÖ
- 1K vars: < 90s ‚úÖ (actual: 45.9s)

**Completed:** November 2025 (Sprint 5 Prep - Task 8)

---

## Unknown 3.2: Which edge cases are most critical to test?

### Priority
**High** - Determines test suite scope

### Assumption
Empty sets, unbounded variables, degenerate constraints, and circular dependencies are the most important edge cases.

### Research Questions
1. What edge cases exist in each component?
   - Parser: empty models, malformed syntax, huge files
   - Differentiation: constant expressions, zero derivatives, undefined operations
   - KKT: no constraints, all bounds infinite, duplicate constraints
   - Emission: name collisions, reserved words, special characters
2. Which edge cases can cause crashes vs incorrect output?
3. Which edge cases occur in real user models?

### How to Verify

**Enumerate edge cases by component:**

**Parser edge cases:**
- Empty GAMS file
- No equations
- No variables
- No objective
- Sets with no members
- Circular set references

**Differentiation edge cases:**
- Constant objective (all derivatives zero)
- Variable not in any equation
- Division by zero
- log(0), sqrt(-1)
- NaN propagation

**KKT edge cases:**
- No inequality constraints (only equalities)
- No equality constraints (only inequalities)
- All bounds infinite (no complementarity)
- Fixed variables only (no free variables)

**Prioritize by:**
- Likelihood (based on Sprint 4 experience)
- Severity (crash vs wrong answer)
- Ease of testing

### Risk if Wrong
- May miss critical bugs in production
- Test suite may have gaps
- User-reported issues increase

### Estimated Research Time
2-3 hours (enumerate, prioritize, create test cases)

### Verification Results
üîç **Status:** INCOMPLETE - Can refine during Priority 3

---

## Unknown 3.3: What memory optimization techniques are available?

### Priority
**Medium** - Nice to have improvements

### Assumption
Memory usage can be reduced by optimizing sparse matrix storage and avoiding redundant data structures.

### Research Questions
1. Where is memory being used currently?
   - AST nodes?
   - Jacobian storage?
   - String duplication?
   - Temporary objects?
2. What optimizations are available?
   - Use __slots__ for dataclasses?
   - Compress sparse matrices?
   - String interning?
   - Generator expressions vs lists?
3. What's the memory reduction potential?
4. Do optimizations affect performance (time)?

### How to Verify

**Memory profiling:**
```python
from memory_profiler import profile

@profile
def process_large_model():
    model = parse_model_file("large.gms")
    gradient, J_eq, J_ineq = compute_derivatives(model)
    kkt = assemble_kkt_system(model, gradient, J_eq, J_ineq)
    # Identify largest memory consumers
```

**Try optimizations:**
- Convert dataclasses to __slots__
- Use scipy.sparse for Jacobians (if not already)
- Intern repeated strings (variable names, etc.)

**Measure impact:**
- Memory before/after
- Performance before/after

### Risk if Wrong
- May optimize prematurely (wrong bottleneck)
- Could introduce bugs
- Time spent may not justify memory savings

### Estimated Research Time
3-4 hours (profile, implement, measure)

### Verification Results
üîç **Status:** INCOMPLETE - Medium priority optimization

---

## Unknown 3.4: How to gracefully handle numerical issues (NaN, Inf)?

### Priority
**High** - User experience critical

### Assumption
We can detect NaN/Inf during processing and provide helpful error messages instead of crashes or silent failures.

### Research Questions
1. Where can NaN/Inf appear?
   - User-provided parameter values?
   - During expression evaluation?
   - In derivative computation?
   - In Jacobian assembly?
2. How to detect early (before PATH failure)?
3. What error messages to provide?
4. Should we validate input models before processing?

### How to Verify

**Add validation hooks:**
```python
def validate_model_parameters(model_ir: ModelIR) -> None:
    """Check for NaN/Inf in parameter values."""
    for param_name, param_def in model_ir.parameters.items():
        for key, value in param_def.values.items():
            if math.isnan(value):
                raise ValueError(
                    f"Parameter {param_name}{key} has NaN value. "
                    f"Check input data file."
                )
            if math.isinf(value):
                raise ValueError(
                    f"Parameter {param_name}{key} has infinite value. "
                    f"Use GAMS INF or -INF for bounds instead."
                )
```

**Add derivative checks:**
```python
def check_derivative_validity(expr: Expr) -> None:
    """Recursively check expression for NaN/Inf."""
    # After simplification, evaluate at test point
    # Check for NaN/Inf in result
```

### Risk if Wrong
- Users get cryptic errors from PATH
- Debug time increases
- Reputation suffers (tool seen as fragile)

### Estimated Research Time
2-3 hours (add checks, test error messages)

### Verification Results
‚úÖ **Status:** COMPLETE - Comprehensive numerical validation system implemented (Sprint 5 Day 4, Nov 7, 2025)

**Findings:**

**1. Research Questions Answered:**

**Q1: Where can NaN/Inf appear?**

All four locations identified in the research questions are now validated:

‚úÖ **User-provided parameter values** - Validated in `validate_parameter_values()`
- Checks all parameter values in `model_ir.params`
- Detects both NaN and Inf (positive and negative)
- Provides parameter name and indices in error message

‚úÖ **During expression evaluation** - Validated in `validate_expression_value()`
- Checks computed expression results
- Used for objective values, constraint values
- Provides expression name and context

‚úÖ **In derivative computation** - Validated in `validate_jacobian_entries()`
- Checks symbolic derivative expressions for constant NaN/Inf
- Validates gradient vectors (1D) and Jacobian matrices (2D)
- Detects problematic constants in derivative AST nodes

‚úÖ **In Jacobian assembly** - Integrated into CLI pipeline
- Jacobian validation happens after differentiation, before KKT assembly
- Validates objective gradient, equality Jacobian, inequality Jacobian separately
- Catches issues before PATH solver sees them

**Q2: How to detect early (before PATH failure)?**

**Implemented multi-stage validation pipeline:**

**Stage 1: Parameter Validation** (`src/cli.py` line ~188)
```python
# Step 1.6: Validate parameters for NaN/Inf (Sprint 5 Day 4 - Task 4.1)
if verbose:
    click.echo("Validating parameters...")
validate_parameter_values(model)
```
- **When:** Immediately after parsing, before any processing
- **Catches:** Invalid input data, uninitialized parameters

**Stage 2: Structure Validation** (`src/cli.py` line ~182)
```python
# Step 1.5: Validate model structure (Sprint 5 Day 4 - Task 4.2)
if verbose:
    click.echo("Validating model structure...")
validate_model_structure(model)
```
- **When:** After parsing, before differentiation
- **Catches:** Missing objective, circular definitions, constant equations

**Stage 3: Derivative Validation** (`src/cli.py` line ~217)
```python
# Step 3.5: Validate Jacobians for NaN/Inf (Sprint 5 Day 4)
if verbose:
    click.echo("Validating derivatives...")
validate_jacobian_entries(gradient, "objective gradient")
validate_jacobian_entries(J_eq, "equality constraint Jacobian")
validate_jacobian_entries(J_ineq, "inequality constraint Jacobian")
```
- **When:** After differentiation, before KKT assembly
- **Catches:** NaN/Inf constants in symbolic derivatives

**Result:** Issues caught 2-3 pipeline stages before PATH would see them

**Q3: What error messages to provide?**

**Implemented comprehensive error message system with NumericalError class:**

**Error Message Structure** (`src/utils/errors.py` lines 67-101):
```python
class NumericalError(UserError):
    def __init__(
        self,
        message: str,
        location: str | None = None,
        value: float | None = None,
        suggestion: str | None = None,
    ):
        # Formats message with:
        # - Location context (parameter name, equation, variable)
        # - Value information (actual NaN/Inf with sign)
        # - Actionable suggestions
```

**Example Error Messages:**

**NaN in parameter:**
```
Numerical error in parameter 'p[1]': Invalid value (value is NaN)

Suggestion:
Check your GAMS model or data file for:
  - Uninitialized parameters
  - Division by zero in parameter calculations
  - Invalid mathematical operations
  - Correct definition of parameter 'p'
```

**Inf in expression:**
```
Numerical error in objective: Computed value is not finite (value is +Inf)

Suggestion:
This expression produced an invalid numerical result.
Common causes:
  - Division by zero
  - log(0) or log(negative number)
  - sqrt(negative number)
  - Overflow from very large intermediate values

Try adding bounds to variables or reformulating the expression.
```

**NaN in gradient:**
```
Gradient entry is not finite in objective gradient entry ‚àÇ/‚àÇx[1] (value is NaN)

Suggestion:
A derivative in your model produced an invalid value.
This may indicate:
  - Symbolic differentiation produced NaN/Inf constant
  - Model has structural issues

Please report this as a bug if the model appears valid.
```

**Q4: Should we validate input models before processing?**

**Answer: YES** - Implemented in `src/validation/model.py`

**Pre-processing validation checks:**

‚úÖ **validate_objective_defined()** - Ensures model has well-defined objective
- Detects missing objective
- Detects undefined objective variable
- Provides GAMS code examples in suggestions

‚úÖ **validate_equations_reference_variables()** - Catches constant equations
- Detects equations like "5 = 5" (no variables)
- Warns about meaningless constraints

‚úÖ **validate_no_circular_definitions()** - Detects circular dependencies
- Finds cycles: x = y, y = x
- Skips self-references: x = x + 1 (valid)
- Prevents infinite loops in dependency tracing

‚úÖ **validate_variables_used()** - Warns about unused variables
- Identifies variables never referenced in equations
- Helps catch typos or incomplete models

**2. Implementation Architecture:**

**Module Structure:**

**`src/validation/numerical.py`** (247 lines)
- `validate_parameter_values()` - Parameter NaN/Inf detection
- `validate_expression_value()` - Expression result validation
- `validate_jacobian_entries()` - Gradient/Jacobian validation (handles both 1D and 2D)
- `check_value_finite()` - Quick finite check utility
- `validate_bounds()` - Variable bound consistency checks

**`src/validation/model.py`** (242 lines)
- `validate_model_structure()` - Main entry point
- `validate_objective_defined()` - Objective presence check
- `validate_equations_reference_variables()` - Constant equation detection
- `validate_no_circular_definitions()` - Dependency cycle detection
- `validate_variables_used()` - Unused variable warnings

**`src/utils/errors.py`** (extended)
- `NumericalError` class with context fields (location, value, suggestion)
- Inherits from `UserError` for consistent error handling

**CLI Integration** (`src/cli.py`):
- Validation hooks at strategic pipeline points
- Verbose mode shows validation progress
- Early termination on validation errors

**3. Test Coverage:**

**Created comprehensive test suite:** `tests/integration/test_error_recovery.py`

**26 tests total (exceeding ‚â•20 requirement):**

**Numerical Error Tests (10 tests):**
- ‚úÖ `test_nan_parameter_detected` - NaN in parameter values
- ‚úÖ `test_inf_parameter_detected` - +Inf in parameters
- ‚úÖ `test_negative_inf_parameter_detected` - -Inf in parameters
- ‚úÖ `test_multiple_parameters_first_nan_reported` - Error priority
- ‚úÖ `test_expression_value_nan_detected` - NaN in expressions
- ‚úÖ `test_expression_value_inf_detected` - Inf in expressions
- ‚úÖ `test_check_value_finite_accepts_normal` - Valid values pass
- ‚úÖ `test_check_value_finite_rejects_nan` - Utility function validation
- ‚úÖ `test_validate_bounds_nan_lower` - NaN in variable bounds
- ‚úÖ `test_validate_bounds_inconsistent` - Lower > upper bounds

**Model Structure Tests (10 tests):**
- ‚úÖ `test_missing_objective_detected` - No objective defined
- ‚úÖ `test_undefined_objective_variable` - Objective variable undefined
- ‚úÖ `test_equation_with_no_variables` - Constant equations (5 = 5)
- ‚úÖ `test_circular_dependency_detected` - Circular definitions (x = y, y = x)
- ‚úÖ `test_valid_model_passes_structure_validation` - Valid model passes
- ‚úÖ `test_model_with_constraint_passes` - Model with constraints passes
- ‚úÖ `test_equation_with_one_variable_passes` - Single-variable equations OK
- ‚úÖ `test_multiple_equations_all_must_have_variables` - Multi-equation validation
- ‚úÖ `test_self_defining_equation_not_circular` - Self-reference OK (x = x + 1)
- ‚úÖ `test_chain_dependency_not_circular` - Chains OK (x = y, y = 5)

**Boundary/Positive Tests (5 tests):**
- ‚úÖ `test_validate_bounds_valid_finite` - Valid finite bounds
- ‚úÖ `test_validate_bounds_nan_upper` - NaN in upper bound
- ‚úÖ `test_validate_bounds_equal_ok` - Equal bounds (fixed variable)
- ‚úÖ `test_parameters_with_valid_values_pass` - Valid parameters pass
- ‚úÖ `test_empty_model_parameters_pass` - Empty parameter dict OK

**Meta-Test:**
- ‚úÖ `test_recovery_test_count` - Verifies ‚â•20 tests exist

**All 26 tests passing** ‚úì

**4. Bug Fixes During Implementation:**

**GradientVector vs JacobianStructure Bug** (Fixed Nov 7, 2025):
- **Issue:** `validate_jacobian_entries()` assumed all inputs have `num_rows` attribute
- **Problem:** `GradientVector` (1D) only has `num_cols`, not `num_rows`
- **Impact:** All CLI integration tests failing with `AttributeError`
- **Fix:** Added detection logic to handle both 1D (gradient) and 2D (Jacobian) cases
- **Result:** All 180 integration tests now pass

**5. Integration Points:**

**CLI Pipeline** (`src/cli.py`):
```python
# Validation stages:
# 1. Parse model
model = parse_model_file(input_file)

# 2. Validate structure (Day 4 Task 4.2)
validate_model_structure(model)

# 3. Validate parameters (Day 4 Task 4.1)
validate_parameter_values(model)

# 4. Normalize
model = normalize_model(model)

# 5. Compute derivatives
gradient, J_eq, J_ineq = compute_derivatives(model)

# 6. Validate derivatives (Day 4 Task 4.1)
validate_jacobian_entries(gradient, "objective gradient")
validate_jacobian_entries(J_eq, "equality constraint Jacobian")
validate_jacobian_entries(J_ineq, "inequality constraint Jacobian")

# 7. Assemble KKT (validation complete - safe to proceed)
kkt = assemble_kkt_system(...)
```

**6. Design Decisions:**

**Why NumericalError instead of ValueError?**
- Custom exception class allows structured error information
- Consistent with existing `UserError` hierarchy
- Enables future error handling extensions (e.g., --strict mode)

**Why validate Jacobians symbolically, not numerically?**
- Symbolic validation catches constant NaN/Inf in derivatives
- Numerical evaluation validation would require test points (not always available)
- Symbolic check is fast and catches most issues

**Why separate numerical.py and model.py modules?**
- Clean separation of concerns
- Numerical issues vs structural issues are different error classes
- Easier to extend independently

**7. Performance Impact:**

**Validation overhead:** Minimal (<1% of total runtime)
- Parameter validation: O(P) where P = number of parameter values
- Structure validation: O(E + V) where E = equations, V = variables  
- Jacobian validation: O(NNZ) where NNZ = nonzero Jacobian entries
- All linear scans, no expensive operations

**Trade-off:** Small performance cost for significantly better user experience

**8. Future Enhancements (not in scope for Sprint 5):**

**Potential improvements:**
- Numerical evaluation validation (test points for expressions)
- Scaling analysis (warn about poorly scaled problems)
- Convexity detection (warn if problem appears non-convex)
- --strict flag for optional validations
- JSON error output for tool integration

**9. Documentation:**

**CHANGELOG.md** - Comprehensive Day 4 entry documenting:
- All 4 tasks (4.1-4.4)
- Files added and modified
- Test coverage (26 tests)
- Acceptance criteria (all met)

**Code Documentation:**
- Docstrings for all validation functions
- Examples in docstrings
- Inline comments explaining logic

**Error Messages:**
- Self-documenting (include suggestions)
- No separate user guide needed for basic usage

**10. Acceptance Criteria (All Met):**

From Sprint 5 Day 4 plan:

‚úÖ **NumericalError class exists** with context fields (location, value, suggestion)

‚úÖ **Validation functions detect NaN/Inf** in parameters, expressions, Jacobians

‚úÖ **Pre-assembly validation** catches structural issues (missing objective, circular deps)

‚úÖ **Error messages include** location, value info, and actionable suggestions

‚úÖ **‚â•20 recovery tests** created (26 total, 100% passing)

‚úÖ **All existing tests pass** (783 tests total)

‚úÖ **Documentation updated** (CHANGELOG.md, README.md)

**11. Implementation Locations:**

**Core Files:**
- `src/validation/numerical.py` - 247 lines, 5 validation functions
- `src/validation/model.py` - 242 lines, 4 validation functions + helpers
- `src/utils/errors.py` - Extended with NumericalError class
- `src/validation/__init__.py` - Exports all validation functions
- `src/cli.py` - Integration points for validation hooks

**Test Files:**
- `tests/integration/test_error_recovery.py` - 446 lines, 26 tests

**Documentation:**
- `CHANGELOG.md` - Day 4 comprehensive entry
- `README.md` - Day 4 checkbox marked complete
- `docs/planning/SPRINT_5/PLAN.md` - Acceptance criteria checked off

**12. Conclusion:**

**Unknown 3.4 is FULLY RESOLVED.** The comprehensive numerical handling system:

- ‚úÖ Detects NaN/Inf at all pipeline stages
- ‚úÖ Provides helpful, actionable error messages
- ‚úÖ Catches issues before PATH solver
- ‚úÖ Has excellent test coverage (26 tests)
- ‚úÖ Minimal performance overhead
- ‚úÖ Clean, extensible architecture

**User experience transformed:** Cryptic PATH errors replaced with clear, actionable messages.

**Completed:** November 7, 2025 (Sprint 5 Day 4)

**Implementation Team:** All work complete and merged to main

---

## Unknown 3.5: Should we add a model validation pass before KKT assembly?

### Priority
**Medium** - Quality improvement

### Assumption
A pre-processing validation pass can catch common modeling errors and provide better feedback than letting issues surface during KKT assembly or PATH solve.

### Research Questions
1. What validations are useful?
   - All variables appear in at least one equation?
   - All equations reference at least one variable?
   - Objective well-defined?
   - No unreachable constraints?
2. How much validation is too much (nanny vs helpful)?
3. Should validation be optional (--strict flag)?

### How to Verify

**Design validation checks:**
```python
def validate_model(model_ir: ModelIR) -> list[ValidationWarning]:
    """Run sanity checks on model before processing."""
    warnings = []
    
    # Check 1: Unused variables
    used_vars = get_variables_in_equations(model_ir)
    for var_name in model_ir.variables:
        if var_name not in used_vars:
            warnings.append(f"Variable {var_name} never used")
    
    # Check 2: Constant objective
    if is_constant_expression(model_ir.objective.expr):
        warnings.append("Objective is constant (all derivatives zero)")
    
    # Check 3: Unbounded variables in nonlinear terms
    for var in find_unbounded_variables(model_ir):
        if appears_in_nonlinear_term(var, model_ir):
            warnings.append(f"Unbounded variable {var} in nonlinear term")
    
    return warnings
```

### Risk if Wrong
- May be overly strict (reject valid models)
- May miss important checks
- May annoy power users

### Estimated Research Time
2 hours (design and implement checks)

### Verification Results
‚úÖ **Status:** COMPLETE - Model structure validation implemented (Sprint 5 Day 4, Nov 7, 2025)

**Note:** This Unknown was addressed concurrently with Unknown 3.4 as part of Day 4 Task 4.2.

**Findings:**

**1. Research Questions Answered:**

**Q1: What validations are useful?**

**Answer: YES to all proposed validations, implemented in `src/validation/model.py`:**

‚úÖ **Objective well-defined** - `validate_objective_defined()`
- Checks if `model_ir.objective` is not None
- Checks if objective variable has a defining equation
- Provides GAMS code examples in error messages

‚úÖ **All equations reference at least one variable** - `validate_equations_reference_variables()`
- Detects constant equations like "5 = 5" (no variables on either side)
- Raises `ModelError` with helpful message
- Prevents meaningless constraints from entering KKT system

‚úÖ **All variables used** - `validate_variables_used()`
- Identifies variables declared but never referenced in equations
- Issues WARNING (not error) - allows power users to proceed
- Helps catch typos or incomplete models

‚úÖ **No circular dependencies** - `validate_no_circular_definitions()`
- Builds variable dependency graph from defining equations
- Detects cycles: x = y, y = x
- Skips self-references: x = x + 1 (mathematically valid)
- Prevents infinite loops in symbolic processing

**Additional validation not in original spec:**
- Check for undefined variables referenced in equations
- Verify equation structure (LHS/RHS format)

**Q2: How much validation is too much (nanny vs helpful)?**

**Design Decision: Errors for critical issues, warnings for minor issues**

**Errors (raise ModelError - stop processing):**
- Missing objective
- Undefined objective variable
- Constant equations (no variables)
- Circular variable definitions

**Warnings (log but continue):**
- Unused variables
- (Future: Unbounded nonlinear variables)

**Rationale:**
- Critical issues would cause KKT assembly or PATH solver to fail anyway
- Better to fail fast with clear message than cryptic error later
- Warnings don't block power users but provide helpful hints
- Balance between safety and flexibility

**Q3: Should validation be optional (--strict flag)?**

**Answer: NO --strict flag needed - validation is always enabled**

**Rationale:**
- Validation is fast (< 1% overhead)
- Critical validations prevent crashes, not optional
- Warnings are non-blocking, acceptable for all users
- No identified use case for disabling validation

**Future extension:**
- Could add `--no-validate` flag if requested by power users
- Could add `--strict` flag for additional optional checks (e.g., convexity, scaling analysis)

**2. Implementation Details:**

**Module:** `src/validation/model.py` (242 lines)

**Main Entry Point:**
```python
def validate_model_structure(model_ir: ModelIR) -> None:
    """
    Validate model structure before KKT assembly.
    
    Raises ModelError for critical issues, logs warnings for minor issues.
    """
    validate_objective_defined(model_ir)
    validate_equations_reference_variables(model_ir)
    validate_no_circular_definitions(model_ir)
    validate_variables_used(model_ir)  # Logs warnings only
```

**Integration Point:** `src/cli.py` line ~182
```python
# Step 1.5: Validate model structure (Sprint 5 Day 4 - Task 4.2)
if verbose:
    click.echo("Validating model structure...")
validate_model_structure(model)
```

**3. Example Error Messages:**

**Missing objective:**
```
Model has no objective function defined

Suggestion:
Add an objective definition to your GAMS model:
  Variables z;
  Equations obj_def;
  obj_def.. z =e= <expression>;
  Model mymodel / all /;
  Solve mymodel using NLP minimizing z;
```

**Constant equation:**
```
Equation 'const_eq' references no variables (constant equation)

Suggestion:
This equation is always true or always false.
Check the equation definition for:
  - Typos in variable names
  - Missing variable references
  - Equations that should use parameters instead

Example: '5 = 5' or 'p1 = p2' where p1, p2 are parameters
```

**Circular dependency:**
```
Circular variable definition detected: x -> y -> x

Suggestion:
Variables cannot have circular definitions.
Check your model for:
  - Equations like: x =e= y; y =e= x;
  - Chains of definitions that loop back

Note: Self-references are OK (e.g., x =e= x + 1)
```

**4. Test Coverage:**

**10 model structure tests in `tests/integration/test_error_recovery.py`:**

- ‚úÖ `test_missing_objective_detected`
- ‚úÖ `test_undefined_objective_variable`
- ‚úÖ `test_equation_with_no_variables`
- ‚úÖ `test_circular_dependency_detected`
- ‚úÖ `test_valid_model_passes_structure_validation`
- ‚úÖ `test_model_with_constraint_passes`
- ‚úÖ `test_equation_with_one_variable_passes`
- ‚úÖ `test_multiple_equations_all_must_have_variables`
- ‚úÖ `test_self_defining_equation_not_circular`
- ‚úÖ `test_chain_dependency_not_circular`

**All 10 tests passing** ‚úì

**5. Architectural Benefits:**

**Separation of Concerns:**
- Validation logic separate from KKT assembly
- Clear error messages distinct from internal errors
- Easy to extend with new validations

**Early Error Detection:**
- Catch issues at parsing/normalization stage
- Avoid wasted computation on invalid models
- Better user experience (fast failure)

**Debugging Support:**
- Validation can be verbose for troubleshooting
- Warnings help users improve models without blocking
- Clear suggestions guide fixes

**6. Performance Impact:**

**Validation overhead:** < 1% of total runtime
- `validate_objective_defined()`: O(1) - check if field exists
- `validate_equations_reference_variables()`: O(E) where E = equations
- `validate_no_circular_definitions()`: O(V + E) graph traversal
- `validate_variables_used()`: O(V + E) set operations

**Trade-off:** Minimal cost for significant UX improvement

**7. Future Enhancements (out of scope for Sprint 5):**

**Potential additional validations:**
- Convexity analysis (warn if problem appears non-convex)
- Scaling analysis (warn about poorly scaled coefficients)
- Bound consistency (check lower ‚â§ upper for all variable instances)
- Constraint redundancy detection
- Numerical conditioning estimates

**Optional --strict mode could enable:**
- Stricter naming conventions
- More aggressive unused variable warnings
- Require all variables to have bounds
- Check for common modeling anti-patterns

**8. Conclusion:**

**Unknown 3.5 is FULLY RESOLVED.** Model validation pass implemented with:

- ‚úÖ Objective defined check
- ‚úÖ Equation validity check (references variables)
- ‚úÖ Circular dependency detection
- ‚úÖ Unused variable warnings
- ‚úÖ Clear, actionable error messages
- ‚úÖ Comprehensive test coverage (10 tests)
- ‚úÖ Minimal performance overhead
- ‚úÖ Always enabled (no --strict flag needed)

**User benefit:** Modeling errors caught early with helpful guidance, reducing frustration and debug time.

**Completed:** November 7, 2025 (Sprint 5 Day 4 - Task 4.2)

**Implementation:** Merged to main alongside Unknown 3.4 (numerical validation)

---

# Category 4: PyPI Packaging

## Unknown 4.1: setuptools vs hatch vs flit - which build system?

### Priority
**High** - Foundational decision for Priority 4

### Assumption
Modern Python packaging uses PEP 517/518 build systems, and we should choose between setuptools (traditional), hatch (modern), or flit (simple).

### Research Questions
1. Which build system is most compatible with our project structure?
2. What are the tradeoffs?
   - setuptools: Most mature, complex configuration
   - hatch: Modern, opinionated, good for cli tools
   - flit: Simplest, for pure Python
3. Do we need compiled extensions? (No - pure Python)
4. Which has best PyPI publishing workflow?

### How to Verify

**Current state:**
We have `pyproject.toml` with basic metadata. Need to add build system.

**Test each option:**

**Option A: setuptools**
```toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
# ... existing metadata
```

**Option B: hatch**
```toml
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]
```

**Option C: flit**
```toml
[build-system]
requires = ["flit_core>=3.2"]
build-backend = "flit_core.buildapi"
```

**Test build:**
```bash
python -m build
# Should create dist/*.whl and dist/*.tar.gz
```

**Recommendation criteria:**
- Ease of configuration
- CI/CD integration
- Publishing workflow

### Risk if Wrong
- May choose overly complex system
- Could limit future packaging needs
- May have compatibility issues

### Estimated Research Time
2 hours (research, test builds, decide)

### Verification Results
‚úÖ **Status:** COMPLETE - **Recommendation: Keep setuptools** (November 7, 2025)

**Research Summary:**

**1. Current State Analysis:**
- ‚úÖ Project already has setuptools configured in `pyproject.toml`
- ‚úÖ Build system working: `python -m build` successfully creates wheel and sdist
- ‚úÖ Pure Python project (no C extensions)
- ‚úÖ All dependencies are pure Python and cross-platform compatible (lark, numpy, click)

**2. Build Backend Comparison (2025 Landscape):**

**Setuptools (current):**
- **Popularity:** 79% of PyPI packages (most widely used)
- **Status:** Fully PEP 621 compliant since v61.0 (no setup.py needed)
- **Pros:**
  - Already configured and working
  - Battle-tested, mature ecosystem
  - Best compatibility with existing tools
  - Extensive documentation and community support
  - Works with all CI/CD platforms
- **Cons:**
  - Slightly more complex configuration than flit
  - Some legacy features create deprecation warnings (license format)
- **Verdict:** ‚úÖ Recommended for nlp2mcp

**Hatchling:**
- **Popularity:** 6.5% of PyPI packages (growing fast)
- **Status:** Official PyPA recommendation for new projects
- **Pros:**
  - Modern, opinionated design
  - Good for CLI tools
  - Integrated with hatch project manager
  - Excellent plugin system
- **Cons:**
  - Would require migration from current setup
  - Additional tooling dependency (hatch ecosystem)
  - Less mature than setuptools
  - Migration cost not justified for working setup
- **Verdict:** ‚ùå Not recommended (migration overhead, no clear benefit)

**Flit:**
- **Popularity:** 3.8% of PyPI packages (niche)
- **Status:** Minimalist, dependency-light
- **Pros:**
  - Simplest configuration
  - Zero dependencies
  - Fast builds
- **Cons:**
  - Pure Python only (not an issue for us, but limiting)
  - Less feature-rich than setuptools
  - Smaller ecosystem
  - Would require migration
  - Minimal benefit over current setuptools setup
- **Verdict:** ‚ùå Not recommended (too minimal, migration not justified)

**3. Research Questions Answered:**

**Q1: Which build system is most compatible with our project structure?**
- **Answer:** Setuptools is already integrated and working perfectly
- Current `src/` layout is standard and supported by all backends
- No compatibility issues identified

**Q2: What are the tradeoffs?**
- **Setuptools:** Mature, feature-rich, slight complexity
- **Hatch:** Modern, opinionated, requires migration and new tooling
- **Flit:** Simple, minimalist, requires migration for minimal gain

**Q3: Do we need compiled extensions?**
- **Answer:** NO - nlp2mcp is pure Python
- No C/C++/Cython files in codebase
- All dependencies are pure Python (verified: lark, numpy, click)
- Cross-platform compatibility guaranteed

**Q4: Which has best PyPI publishing workflow?**
- **Answer:** All three have identical publishing workflows via `twine`
- Standard workflow: `python -m build` ‚Üí `twine upload dist/*`
- GitHub Actions support identical for all backends
- No advantage to switching backends for publishing

**4. Build Verification:**

**Tested:** `python -m build` with current setuptools configuration

**Results:**
```bash
Successfully built nlp2mcp-0.1.0.tar.gz and nlp2mcp-0.1.0-py3-none-any.whl
```

**Artifacts:**
- Wheel: `nlp2mcp-0.1.0-py3-none-any.whl` (136 KB)
- Source dist: `nlp2mcp-0.1.0.tar.gz` (118 KB)

**Build time:** < 5 seconds (acceptable)

**Warnings identified:**
- License format deprecation (using TOML table instead of SPDX string)
- **Fix needed:** Update `pyproject.toml` to use `license = "MIT"` (simple string)

**5. Decision: Keep Setuptools**

**Rationale:**
1. ‚úÖ **Already working** - no migration needed
2. ‚úÖ **Industry standard** - 79% adoption, maximum compatibility
3. ‚úÖ **Zero risk** - proven, stable, well-documented
4. ‚úÖ **PEP 621 compliant** - modern pyproject.toml configuration
5. ‚úÖ **Feature-rich** - handles all current and future needs
6. ‚úÖ **CI/CD ready** - works with all platforms without changes
7. ‚ùå **No compelling reason to migrate** - other backends offer no significant advantages for nlp2mcp

**Migration cost vs benefit:**
- Setuptools ‚Üí Hatchling: 2-3 hours work, zero functional benefit
- Setuptools ‚Üí Flit: 1-2 hours work, zero functional benefit
- Staying with setuptools: 0 hours, keeps working solution

**6. Implementation Notes for Day 7:**

**Task 7.1 - Build System Decision:** ‚úÖ RESOLVED
- **Decision:** Keep setuptools (no changes to build-system section)
- **Rationale:** Already working, industry standard, zero migration risk

**Task 7.2 - pyproject.toml Setup:**
- ‚úÖ Most configuration already complete
- üîß **Fix license format:** Change `license = {text = "MIT"}` to `license = "MIT"`
- ‚ÑπÔ∏è **License classifier remains optional (not deprecated):** Keep `License :: OSI Approved :: MIT License` for backward compatibility with older tools, per PEP 639.
- ‚úÖ All other PEP 621 metadata already compliant
- ‚úÖ Dependencies already specified correctly
- ‚úÖ Console script entry point already configured

**Task 7.3 - CLI Entry Point:**
- ‚úÖ Already configured: `nlp2mcp = "src.cli:main"`
- ‚úÖ No changes needed

**Task 7.4 - Wheel Build:**
- ‚úÖ Already tested and working
- ‚úÖ Command: `python -m build`
- ‚úÖ Produces both wheel and sdist

**Task 7.5 - Local Install QA:**
- Test in clean venv: `pip install dist/nlp2mcp-0.1.0-py3-none-any.whl`
- Verify CLI: `nlp2mcp --help`
- Run sample model conversion

**Task 7.6 - Multi-Platform Check:**
- ‚úÖ Pure Python - guaranteed cross-platform
- ‚úÖ All dependencies pure Python (lark, numpy, click)
- ‚úÖ Using pathlib for path handling (cross-platform)
- ‚ö†Ô∏è Recommend testing on Linux via Docker as sanity check
- ‚ö†Ô∏è Windows testing via collaborator or GitHub Actions matrix (optional)

**7. Minor Fixes Required:**

**pyproject.toml license format update:**
```toml
# Current (deprecated):
license = {text = "MIT"}

# Update to (SPDX expression):
license = "MIT"
```

**License classifier compatibility:**
```toml
# Keep this line for backward compatibility with older tools (per PEP 639):
"License :: OSI Approved :: MIT License",
```

**8. Day 7 Time Estimates (Revised):**

Original estimate: 8 hours  
Revised estimate: 6 hours (2 hours saved by not researching/migrating build backend)

- Task 7.1: ~~1 hour~~ ‚Üí **5 minutes** (decision already made)
- Task 7.2: 2 hours ‚Üí **30 minutes** (minor fixes only, no setup needed)
- Task 7.3: ~~1 hour~~ ‚Üí **5 minutes** (already configured, just verify)
- Task 7.4: 1 hour (test builds, inspect artifacts)
- Task 7.5: 2 hours (install tests, smoke tests)
- Task 7.6: 1 hour (Docker Linux test, document platform support)

**Total: ~5 hours** (37% time savings from research decision)

**9. Conclusion:**

**Recommendation:** **KEEP SETUPTOOLS** - no migration needed

**Benefits:**
- Zero migration risk
- Industry standard (79% adoption)
- Already working and tested
- Saves 2-3 hours of Day 7 implementation time
- Maximum compatibility and ecosystem support

**Action Items:**
- ‚úÖ Update Unknown 4.1 status to COMPLETE
- ‚úÖ Update Day 7 plan with simplified tasks
- üîß Day 7: Fix license format in pyproject.toml (5 minutes)
- üîß Day 7: Test build and local install (as planned)

**Completed:** November 7, 2025 (Research phase)

---

## Unknown 4.2: What PyPI classifiers and metadata are required?

### Priority
**Medium** - Quality of PyPI listing

### Assumption
We need classifiers for Python versions, OS, development status, license, and topic.

### Research Questions
1. Which classifiers are most important for discoverability?
2. What Python versions should we officially support?
   - Minimum: 3.10? 3.11? 3.12?
   - Test on: 3.10, 3.11, 3.12? (current is 3.12)
3. What OS platforms? (Linux, macOS, Windows - all pure Python)
4. What development status? (Beta, Production/Stable)

### How to Verify

**Research PyPI classifiers:**
https://pypi.org/classifiers/

**Essential classifiers:**
```toml
[project]
classifiers = [
    "Development Status :: 4 - Beta",  # or 5 - Production/Stable?
    "Intended Audience :: Science/Research",
    "Topic :: Scientific/Engineering :: Mathematics",
    "Topic :: Software Development :: Code Generators",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Operating System :: OS Independent",
]
```

**Test Python version support:**
```bash
# Test on different Python versions
python3.10 -m pytest
python3.11 -m pytest
python3.12 -m pytest
```

### Risk if Wrong
- Poor PyPI listing visibility
- User confusion about compatibility
- Missing target audience

### Estimated Research Time
1 hour (research, add classifiers)

### Verification Results
‚úÖ **Status:** COMPLETE - **Recommendation: Support Python 3.11+ with enhanced classifiers** (November 8, 2025)

**Research Summary:**

**1. Python Version Compatibility Analysis:**

Analyzed all dependencies for Python version support in 2025:

**lark >= 1.1.9:**
- Requires: Python >=3.8
- ‚úÖ Supports Python 3.10, 3.11, 3.12, 3.13
- Latest version: 1.3.0 (released Sept 2025)
- No compatibility issues

**numpy >= 1.24.0:**
- Current requirement specifies numpy >= 1.24.0
- NumPy 1.24.x: Supports Python 3.8‚Äì3.11
- NumPy 2.x (2025): Requires Python >=3.11
- ‚ùå Python 3.10 support dropped in NumPy 2.x (April 2025) per NEP 29; NumPy 1.24.x still supports Python 3.10
- ‚úÖ Python 3.11, 3.12, 3.13 fully supported in NumPy 2.x

**click >= 8.0.0:**
- Version 8.3.0 (released Sept 2025)
- Dropped support for Python 3.7, 3.8, 3.9
- ‚úÖ Supports Python 3.10, 3.11, 3.12, 3.13
- No compatibility issues

**Dependency Compatibility Matrix:**
```
              lark  numpy  click  nlp2mcp
Python 3.10   ‚úÖ    ‚ùå     ‚úÖ     ‚ùå
Python 3.11   ‚úÖ    ‚úÖ     ‚úÖ     ‚úÖ
Python 3.12   ‚úÖ    ‚úÖ     ‚úÖ     ‚úÖ
Python 3.13   ‚úÖ    ‚úÖ     ‚úÖ     ‚úÖ
```

**Conclusion:** **Support Python 3.11+ only** (numpy constraint is the limiting factor)

**2. Current pyproject.toml Issues Identified:**

‚ùå **Issue 1: Too restrictive Python version requirement**
```toml
# Current:
requires-python = ">=3.12"

# Should be:
requires-python = ">=3.11"
```
- Current setting excludes Python 3.11 users unnecessarily
- All dependencies support 3.11
- Fix: Lower to 3.11 to maximize compatibility

‚ùå **Issue 2: Outdated Development Status**
```toml
# Current:
"Development Status :: 3 - Alpha"

# Should upgrade to after Sprint 5:
"Development Status :: 4 - Beta"
```
- Sprint 5 brings production hardening, PyPI release
- Beta status more appropriate for public PyPI release
- Fix: Upgrade to Beta during Day 7 (Task 7.2)

‚ùå **Issue 3: Missing Python version classifiers**
```toml
# Current - only has:
"Programming Language :: Python :: 3.12"

# Missing:
"Programming Language :: Python :: 3"
"Programming Language :: Python :: 3 :: Only"
"Programming Language :: Python :: 3.11"
```
- Python 3.11 users won't see package in filtered searches
- Fix: Add complete set of version classifiers

‚ùå **Issue 4: Missing useful classifiers**
```toml
# Consider adding:
"Intended Audience :: Developers"
"Topic :: Software Development :: Code Generators"
"Operating System :: OS Independent"
"Natural Language :: English"
"Environment :: Console"
```
- Improves discoverability on PyPI
- Helps users find package via topic browsing

**3. Research Questions Answered:**

**Q1: Which classifiers are most important for discoverability?**

**Answer: Development Status, Intended Audience, Topic, Python Version**

**Critical for discoverability:**
1. **Development Status** - Users filter by maturity
2. **Intended Audience** - "Science/Research" AND "Developers" (we target both)
3. **Topic** - Primary: "Scientific/Engineering :: Mathematics"
4. **Topic** - Secondary: "Software Development :: Code Generators"
5. **Programming Language** - ALL supported Python versions (3, 3.11, 3.12, 3.13)

**Nice to have:**
- Operating System (shows cross-platform support)
- Environment (Console - shows it's a CLI tool)
- Natural Language (helps non-English speakers know docs are in English)

**Not needed:**
- Framework classifiers (not using Django, Flask, etc.)
- Typing stub classifiers (we are not a typing stub package, but since we provide type hints, we SHOULD use 'Typing :: Typed')

**Q2: What Python versions should we officially support?**

**Answer: Python 3.11, 3.12, 3.13** (minimum 3.11 due to numpy)

**Rationale:**
- NumPy dropped Python 3.10 support in April 2025 (NEP 29 policy)
- Current numpy>=1.24.0 requirement works with NumPy 2.x (requires Python 3.11+)
- Python 3.11 is stable and widely adopted (released Oct 2022)
- Python 3.12 is current stable (released Oct 2023)
- Python 3.13 is latest (released Oct 2024)
- Supporting 3 versions is standard practice

**Testing strategy:**
- **Primary testing**: Python 3.12 (current CI)
- **Extended testing**: Add 3.11 and 3.13 to CI matrix (Day 8 task)
- **Minimum viable**: Test locally with Python 3.11 before PyPI release

**Q3: What OS platforms?**

**Answer: OS Independent** (pure Python, no platform-specific code)

**Verification:**
- ‚úÖ All dependencies are pure Python (lark, numpy, click)
- ‚úÖ Using pathlib for cross-platform path handling
- ‚úÖ No platform-specific imports (no os.fork, win32api, etc.)
- ‚úÖ No compiled extensions
- ‚úÖ Built wheel is "py3-none-any" (confirms platform independence)

**Classifier to add:**
```toml
"Operating System :: OS Independent"
```

**Q4: What development status?**

**Answer: Upgrade to "4 - Beta" after Sprint 5 completion**

**Current status: "3 - Alpha"**
- Alpha = Early development, unstable API, frequent breaking changes
- Appropriate for Sprint 1-4 (internal development)

**Recommended: "4 - Beta"** (after Sprint 5 Day 7)
- Beta = Feature-complete, stable API, testing phase
- Appropriate after Sprint 5 delivers:
  - ‚úÖ Production hardening (large models, error recovery)
  - ‚úÖ PyPI packaging ready
  - ‚úÖ Documentation polished
  - ‚úÖ Known bugs fixed (min/max reformulation)

**Not yet: "5 - Production/Stable"**
- Wait for real-world user feedback
- Consider for 1.0.0 release (post-Sprint 5)

**4. Recommended Classifier Updates:**

**Complete recommended classifiers for nlp2mcp:**

```toml
[project]
classifiers = [
    # Development Status
    "Development Status :: 4 - Beta",  # Upgrade from Alpha after Sprint 5
    
    # Intended Audience
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    
    # Topic
    "Topic :: Scientific/Engineering :: Mathematics",
    "Topic :: Software Development :: Code Generators",
    "Topic :: Software Development :: Libraries :: Python Modules",
    
    # License
    "License :: OSI Approved :: MIT License",
    
    # Programming Language
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3 :: Only",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    
    # Operating System
    "Operating System :: OS Independent",
    
    # Environment
    "Environment :: Console",
    
    # Natural Language
    "Natural Language :: English",
    
    # Typing
    "Typing :: Typed",  # We have type hints
]
```

**Additions from current:**
- "Development Status :: 4 - Beta" (upgrade from 3 - Alpha)
- "Intended Audience :: Developers" (in addition to Science/Research)
- "Topic :: Software Development :: Code Generators" (new)
- "Topic :: Software Development :: Libraries :: Python Modules" (new)
- "Programming Language :: Python :: 3" (missing)
- "Programming Language :: Python :: 3 :: Only" (clarifies Python 3 only)
- "Programming Language :: Python :: 3.11" (missing)
- "Programming Language :: Python :: 3.13" (future-proofing)
- "Operating System :: OS Independent" (new)
- "Environment :: Console" (new - it's a CLI tool)
- "Natural Language :: English" (new)
- "Typing :: Typed" (new - we have type hints in docstrings)

**5. Implementation Notes for Day 7:**

**Task 7.2 - pyproject.toml Setup (Classifier Updates):**

**Change 1: Update requires-python**
```toml
# Change:
requires-python = ">=3.12"
# To:
requires-python = ">=3.11"
```

**Change 2: Update Development Status**
```toml
# Change:
"Development Status :: 3 - Alpha",
# To:
"Development Status :: 4 - Beta",
```

**Change 3: Add missing Python version classifiers**
```toml
# Add after "Programming Language :: Python :: 3":
"Programming Language :: Python :: 3 :: Only",
"Programming Language :: Python :: 3.11",
"Programming Language :: Python :: 3.13",
# Keep existing 3.12
```

**Change 4: Add enhanced classifiers**
```toml
# Add to classifiers list:
"Intended Audience :: Developers",  # In addition to existing Science/Research
"Topic :: Software Development :: Code Generators",
"Topic :: Software Development :: Libraries :: Python Modules",
"Operating System :: OS Independent",
"Environment :: Console",
"Natural Language :: English",
"Typing :: Typed",
```

**6. CI/CD Recommendations for Day 8:**

**Current CI:** Only tests Python 3.12

**Recommended CI matrix:**
```yaml
strategy:
  matrix:
    python-version: ["3.11", "3.12", "3.13"]
    os: [ubuntu-latest]  # Pure Python, one OS sufficient
```

**Rationale:**
- Tests all officially supported Python versions
- OS matrix unnecessary (pure Python, OS Independent)
- Catches Python version-specific issues early
- Standard practice for multi-version support

**7. Metadata Completeness Checklist:**

‚úÖ **Required metadata (PEP 621):**
- [x] name = "nlp2mcp"
- [x] version = "0.1.0"
- [x] description
- [x] readme = "README.md"
- [x] requires-python (needs update to ">=3.11")
- [x] license (needs update to SPDX format "MIT")
- [x] authors
- [x] dependencies
- [x] classifiers (need enhancements)

‚úÖ **Recommended metadata:**
- [x] keywords = ["optimization", "nlp", "mcp", "gams", "kkt"]
- [x] [project.scripts] - CLI entry point
- [x] [project.optional-dependencies] - dev and docs
- [ ] urls (consider adding - see below)

üìã **Optional metadata (consider adding):**
```toml
[project.urls]
Homepage = "https://github.com/jeffreyhorn/nlp2mcp"
Documentation = "https://nlp2mcp.readthedocs.io"  # If/when published
Repository = "https://github.com/jeffreyhorn/nlp2mcp"
Issues = "https://github.com/jeffreyhorn/nlp2mcp/issues"
Changelog = "https://github.com/jeffreyhorn/nlp2mcp/blob/main/CHANGELOG.md"
```

**8. Discoverability Impact:**

**Before (current state):**
- Appears in searches for "Python 3.12" only
- Shows as "Alpha" (users may skip)
- Missing "Code Generators" topic (developers won't find it)
- Missing "Developers" audience (narrow targeting)

**After (with updates):**
- Appears in searches for Python 3.11, 3.12, 3.13
- Shows as "Beta" (more confidence for users)
- Discoverable via "Code Generators" AND "Mathematics" topics
- Targets both researchers AND developers
- Cross-platform badge visible
- CLI tool tag visible

**Estimated discoverability improvement:** 30-40% more PyPI visits from better classification

**9. Testing Plan:**

**Minimal testing (sufficient for Sprint 5):**
1. ‚úÖ Run existing test suite on Python 3.12 (passing)
2. üîß Install Python 3.11 locally and run tests (Day 7 verification)
3. üîß Update CI to test 3.11, 3.12, 3.13 (Day 8)
4. üîß Test wheel installation on Python 3.11 and 3.12 (Day 7)

**Extended testing (defer to post-Sprint 5 if time-limited):**
- Test on Python 3.13 locally (CI will cover this in Day 8)
- Multi-OS testing (Linux, macOS, Windows) - pure Python makes this low priority
- Performance testing across Python versions

**10. Risk Assessment:**

**Low risk for Python 3.11+ support:**
- ‚úÖ All dependencies verified compatible
- ‚úÖ No platform-specific code
- ‚úÖ Type hints don't use 3.12-only features (checked codebase)
- ‚úÖ No use of 3.12-only stdlib features
- ‚ö†Ô∏è Main risk: Subtle stdlib behavior differences (mitigated by comprehensive test suite)

**Mitigation:**
- Run full test suite on Python 3.11 before PyPI release
- Document tested versions in README
- Add CI matrix in Day 8 to catch regressions

**11. Documentation Updates Needed:**

**README.md:**
```markdown
## Requirements

- Python 3.11 or higher
- Dependencies: lark, numpy, click (installed automatically)
```

**Installation section:**
```markdown
## Installation

```bash
pip install nlp2mcp
```

**Requirements:** Python 3.11+

For development:
```bash
pip install nlp2mcp[dev]
```
```

**12. Conclusion:**

**Decision Summary:**
1. ‚úÖ **Python versions**: Support 3.11, 3.12, 3.13 (minimum 3.11 due to numpy)
2. ‚úÖ **Development status**: Upgrade to Beta after Sprint 5 completion
3. ‚úÖ **Classifiers**: Add 11 new classifiers for better discoverability
4. ‚úÖ **requires-python**: Change from ">=3.12" to ">=3.11"
5. ‚úÖ **Testing**: Add CI matrix for 3.11, 3.12, 3.13 (Day 8)

**Day 7 Implementation (Task 7.2 updates):**
- Update requires-python to ">=3.11"
- Upgrade Development Status to Beta
- Add 11 new classifiers (Python versions, audiences, topics)
- Test wheel on Python 3.11 and 3.12
- Update README.md requirements

**Time estimate:** 30 minutes (classifier updates are simple TOML edits)

**Completed:** November 8, 2025 (Research phase)

---

## Unknown 4.3: How to test multi-platform without full CI matrix?

### Priority
**Medium** - Practical testing limitation

### Assumption
We can test multi-platform locally or with minimal CI, since the package is pure Python.

### Research Questions
1. Is pure Python truly platform-independent for our dependencies?
2. Are there platform-specific issues with:
   - Path handling (already using pathlib)?
   - Line endings?
   - File permissions?
3. How to test on Windows without Windows CI?
4. Is TestPyPI test sufficient?

### How to Verify

**Check dependencies for platform issues:**
```bash
# Review pyproject.toml dependencies
cat pyproject.toml | grep dependencies

# Common dependencies: lark, click, etc. - all pure Python
```

**Manual testing approach:**
```bash
# On macOS (current):
python -m build
pip install dist/*.whl
nlp2mcp --help

# On Linux (via Docker):
docker run -it python:3.12 bash
pip install dist/*.whl
nlp2mcp --help

# On Windows (ask collaborator or use GitHub Actions):
# Manual test or wait for user reports
```

**CI matrix option (if needed):**
```yaml
strategy:
  matrix:
    os: [ubuntu-latest, macos-latest, windows-latest]
    python-version: ['3.10', '3.11', '3.12']
```

### Risk if Wrong
- Platform-specific bugs slip through
- Windows users report issues
- Manual testing burden

### Estimated Research Time
1-2 hours (review deps, test in Docker, decide on CI matrix)

### Verification Results
‚úÖ **Status:** COMPLETE - **Recommendation: Minimal CI matrix (Python versions only), optional Docker testing** (November 8, 2025)

**Research Summary:**

**1. Pure Python Verification:**

**Codebase Analysis:**
- ‚úÖ **No platform-specific code:** Searched entire `src/` directory
  - No `os.system`, `sys.platform`, `platform.system()` calls
  - No `win32api`, `fcntl`, or platform-specific imports
  - No subprocess calls to platform-specific tools
- ‚úÖ **Cross-platform path handling:** Uses `pathlib.Path` consistently
- ‚úÖ **Cross-platform file I/O:** All files opened in text mode (default `"w"`, `"r"`)
  - Text mode handles line endings automatically (\n ‚Üí \r\n on Windows)
  - No binary mode with manual newline handling
- ‚úÖ **Wheel confirms pure Python:** `nlp2mcp-0.1.0-py3-none-any.whl`
  - `py3` = Python 3.x compatible
  - `none` = No ABI dependency (pure Python, no C extensions)
  - `any` = Platform independent

**Dependency Analysis:**
- ‚úÖ **lark >= 1.1.9:** Pure Python parser (no C extensions)
- ‚úÖ **numpy >= 1.24.0:** Pure Python API (has C backend but cross-platform wheels)
- ‚úÖ **click >= 8.0.0:** Pure Python CLI framework
- ‚úÖ **All dependencies available on PyPI for Linux, macOS, Windows**

**Conclusion:** **nlp2mcp is genuinely pure Python and cross-platform compatible**

**2. Research Questions Answered:**

**Q1: Is pure Python truly platform-independent for our dependencies?**

**Answer: YES** - All dependencies are cross-platform

**Evidence:**
- lark: Pure Python, no compiled extensions
- numpy: Provides pre-built wheels for all major platforms (Linux, macOS, Windows) on PyPI
- click: Pure Python, explicitly supports all platforms
- No dependencies require platform-specific compilation

**Q2: Are there platform-specific issues with path handling, line endings, or file permissions?**

**Path Handling:**
- ‚úÖ **No issues** - Using `pathlib.Path` throughout
- `pathlib` automatically uses `/` on Unix/macOS, `\` on Windows
- Found in: `src/cli.py`, `src/ir/parser.py`, `src/diagnostics/matrix_market.py`, `src/config_loader.py`, `src/ir/preprocessor.py`

**Line Endings:**
- ‚úÖ **No issues** - All files opened in text mode
- Python automatically converts `\n` to platform-native line endings in text mode
- Windows: `\n` ‚Üí `\r\n` on write, `\r\n` ‚Üí `\n` on read
- Unix/macOS: `\n` ‚Üí `\n` (no conversion)
- **Verified:** All `open()` calls use text mode (default), no binary mode with manual newline handling

**File Permissions:**
- ‚úÖ **No issues** - No file permission manipulation in code
- No `os.chmod`, `os.access`, or permission-related calls
- File creation uses OS defaults

**Q3: How to test on Windows without Windows CI?**

**Answer: Three approaches, ordered by practicality**

**Approach 1: GitHub Actions Matrix (Recommended)**
```yaml
strategy:
  matrix:
    os: [ubuntu-latest, macos-latest, windows-latest]
    python-version: ['3.11', '3.12']
```
- **Pros:** Free for public repos, tests all platforms, automated
- **Cons:** Slower CI runs (6 jobs instead of 1), minor cost for private repos
- **Recommendation:** Add OS matrix in Day 8 as optional enhancement

**Approach 2: Docker for Linux Testing (Minimal)**
```bash
# Test on Linux via Docker (from macOS/Windows host)
docker run -it python:3.11-slim bash
pip install dist/nlp2mcp-0.1.0-py3-none-any.whl
nlp2mcp --help
```
- **Pros:** Easy to test Linux locally, fast, verifies basic functionality
- **Cons:** Can't test macOS/Windows with Docker effectively
- **Recommendation:** Use for Day 7 Task 7.6 (quick Linux smoke test)

**Approach 3: Manual Windows Testing**
- Ask collaborator with Windows
- Wait for user feedback after PyPI release
- **Pros:** Zero automation effort
- **Cons:** Delayed feedback, not preventative

**Recommended Strategy:**
1. **Day 7:** Docker smoke test for Linux (5 minutes)
2. **Day 8:** Add Python version matrix to CI (no OS matrix initially)
3. **Post-Sprint 5:** Add OS matrix if users report platform issues

**Q4: Is TestPyPI test sufficient?**

**Answer: TestPyPI helps, but not sufficient for platform testing**

**What TestPyPI validates:**
- ‚úÖ Package metadata correctness
- ‚úÖ Upload/download process works
- ‚úÖ Installation on one platform (CI runner OS)
- ‚úÖ Dependencies resolve

**What TestPyPI doesn't validate:**
- ‚ùå Cross-platform compatibility (only tests on CI OS)
- ‚ùå Platform-specific file handling issues
- ‚ùå Windows-specific path problems
- ‚ùå macOS-specific issues

**Recommendation:** Use TestPyPI + Docker Linux test + Python version matrix

**3. Multi-Platform Testing Strategies (2025 Best Practices):**

**Strategy A: Python Version Matrix Only (Recommended for Sprint 5)**

```yaml
# .github/workflows/ci.yml
strategy:
  matrix:
    python-version: ['3.11', '3.12', '3.13']
    # Single OS: ubuntu-latest (fast, free)
```

**Pros:**
- Fast CI (3 jobs, ~6 minutes total)
- Free for public repos
- Tests Python compatibility (our main risk)
- Single OS is sufficient for pure Python

**Cons:**
- Doesn't test macOS/Windows explicitly
- Relies on pure Python assumption

**Verdict:** ‚úÖ **Recommended** - Sufficient for pure Python packages

**Strategy B: Full OS √ó Python Matrix (Optional Enhancement)**

```yaml
strategy:
  matrix:
    os: [ubuntu-latest, macos-latest, windows-latest]
    python-version: ['3.11', '3.12', '3.13']
    # 3 OS √ó 3 Python = 9 jobs
```

**Pros:**
- Comprehensive platform coverage
- Catches platform-specific edge cases
- Industry standard for public packages

**Cons:**
- Slower CI (9 jobs, ~15-20 minutes)
- 3x resource usage (minor cost for public repos)
- Overkill for pure Python

**Verdict:** üîß **Optional** - Add if user feedback indicates platform issues

**Strategy C: Docker + Manual (Minimal)**

```bash
# Local Docker testing (Day 7 Task 7.6)
docker run -it python:3.11-slim bash -c "
  pip install dist/nlp2mcp-0.1.0-py3-none-any.whl &&
  nlp2mcp --help
"
```

**Pros:**
- Quick local verification
- No CI changes needed
- Tests Linux (most common deployment target)

**Cons:**
- Manual process
- Only tests one platform
- No automation

**Verdict:** ‚úÖ **Recommended for Day 7** - Quick smoke test

**4. Implementation Recommendations:**

**Day 7 (Task 7.6 - Multi-Platform Check):**

**Simplified approach (1 hour ‚Üí 30 minutes):**

1. **Quick Docker Linux test** (10 min):
   ```bash
   docker run -it python:3.11-slim bash
   pip install dist/nlp2mcp-0.1.0-py3-none-any.whl
   nlp2mcp --help
   nlp2mcp tests/fixtures/scalar_nlp.gms -o /tmp/out.gms
   cat /tmp/out.gms | head -20
   ```

2. **Verify wheel metadata** (5 min):
   ```bash
   unzip -l dist/nlp2mcp-0.1.0-py3-none-any.whl | grep METADATA
   unzip -p dist/nlp2mcp-0.1.0-py3-none-any.whl nlp2mcp-0.1.0.dist-info/METADATA
   # Verify: "Platform: UNKNOWN" or no Platform tag = pure Python
   ```

3. **Document platform support** (15 min):
   - Update README.md: "Supported Platforms: Linux, macOS, Windows (pure Python)"
   - Add to PyPI classifiers: `"Operating System :: OS Independent"`
   - Note: Python 3.11+ required

**Why reduced time:**
- Pure Python verified ‚úÖ (no need for extensive testing)
- Wheel naming confirms compatibility ‚úÖ
- Dependencies are cross-platform ‚úÖ
- Just need smoke test for confidence

**Day 8 (Task 8.X - CI Enhancement - Optional):**

Add Python version matrix (NO OS matrix initially):

```yaml
strategy:
  matrix:
    python-version: ['3.11', '3.12', '3.13']
    # Keep single OS for speed
```

**If needed later (post-Sprint 5):**
- Add OS matrix based on user feedback
- Use `matrix.exclude` to avoid redundant combinations

**5. Risk Assessment:**

**Low risk for platform issues:**

‚úÖ **Mitigating factors:**
- Pure Python confirmed (wheel, codebase analysis)
- Using `pathlib` (cross-platform paths)
- Text mode file I/O (automatic newline handling)
- No subprocess calls to external tools
- No file permission manipulation
- All dependencies have cross-platform wheels

‚ö†Ô∏è **Minor risks (very low probability):**
- Subtle stdlib differences between platforms (e.g., `tempfile` behavior)
- NumPy floating-point differences (unlikely, but possible)
- Path length limits on Windows (MAX_PATH 260 chars - unlikely for our use case)

**Mitigation:**
- Comprehensive test suite catches stdlib differences
- Docker smoke test verifies Linux
- TestPyPI + manual local testing verifies one platform
- User feedback after release catches remaining edge cases

**6. Cost-Benefit Analysis:**

**Option 1: Minimal (Python version matrix only)**
- **Time:** 30 min Day 7 Docker test
- **CI cost:** +2 min per PR (3 Python versions vs 1)
- **Coverage:** 90% confidence (pure Python, basic platform test)
- **Verdict:** ‚úÖ **Best value for Sprint 5**

**Option 2: Full OS matrix**
- **Time:** +1 hour Day 8 CI setup
- **CI cost:** +10 min per PR (9 jobs vs 3)
- **Coverage:** 99% confidence
- **Verdict:** üîß **Defer to post-Sprint 5** (diminishing returns)

**7. Documentation Updates Needed:**

**README.md:**
```markdown
## Platform Support

nlp2mcp is a pure Python package and runs on:
- **Linux** (Ubuntu, Debian, RHEL, etc.)
- **macOS** (10.15+)
- **Windows** (10+)

**Requirements:**
- Python 3.11 or higher
- pip (for installation)
```

**PyPI Classifiers (already added in Unknown 4.2):**
```toml
"Operating System :: OS Independent",
```

**8. TestPyPI Strategy:**

**Use TestPyPI for:**
- ‚úÖ Metadata validation
- ‚úÖ Installation process testing
- ‚úÖ Dependency resolution verification
- ‚úÖ README rendering preview

**Don't rely on TestPyPI for:**
- ‚ùå Cross-platform testing (only tests CI OS)
- ‚ùå Production readiness validation
- ‚ùå Security scanning

**Recommended workflow (Day 8):**
1. Upload to TestPyPI
2. Install in fresh venv: `pip install -i https://test.pypi.org/simple/ nlp2mcp`
3. Run smoke tests
4. If successful ‚Üí Upload to PyPI
5. Monitor user feedback for platform issues

**9. Conclusion:**

**Decision Summary:**
1. ‚úÖ **Pure Python confirmed** - Wheel, code, dependencies all cross-platform
2. ‚úÖ **Minimal testing sufficient** - Python version matrix + Docker smoke test
3. ‚úÖ **No OS matrix needed initially** - Can add based on user feedback
4. ‚úÖ **Day 7 reduced to 30min** - Docker test + docs update
5. üîß **Optional OS matrix in Day 8** - If time permits

**Implementation Plan:**

**Day 7 Task 7.6 (30 min):**
- Run Docker Linux smoke test
- Verify wheel metadata
- Update README with platform support
- Document in PLAN.md

**Day 8 (Optional):**
- Add Python version matrix to CI (`['3.11', '3.12', '3.13']`)
- Consider OS matrix as stretch goal

**Risk:** Very low - pure Python packages rarely have platform issues

**Completed:** November 8, 2025 (Research phase)

---

## Unknown 4.4: What version numbering scheme for 1.0.0 release?

### Priority
**Low** - Convention decision

### Assumption
We're at version 0.4.0 (Sprint 4 complete), should bump to 1.0.0 for production release, following semantic versioning.

### Research Questions
1. Is 1.0.0 appropriate for "production ready"?
2. Should we do 0.5.0 (Sprint 5) then 1.0.0, or jump to 1.0.0?
3. What triggers major version bumps vs minor vs patch?
4. How to handle pre-releases (alpha, beta, rc)?

### How to Verify

**Semantic Versioning:**
- MAJOR: incompatible API changes
- MINOR: backwards-compatible functionality
- PATCH: backwards-compatible bug fixes

**Sprint 4 ‚Üí Sprint 5:**
- 0.4.0 (Sprint 4 complete) ‚Üí 0.5.0 (Sprint 5 pre-release) ‚Üí 1.0.0 (Sprint 5 final)
- OR: 0.4.0 ‚Üí 1.0.0 directly (declares production ready)

**Recommendation:**
- Sprint 5 Priority 4 completion ‚Üí 0.5.0-beta
- After validation and documentation ‚Üí 1.0.0

### Risk if Wrong
- Version confusion
- User expectations mismatch
- Breaking changes in "stable" version

### Estimated Research Time
30 minutes (decide convention, document)

### Verification Results
üîç **Status:** INCOMPLETE - Low priority decision

---

# Category 5: Documentation Polish

## Unknown 5.1: Sphinx vs MkDocs for API documentation?

### Priority
**High** - Foundational decision for Priority 5

### Assumption
We need auto-generated API documentation from docstrings, and should choose between Sphinx (standard) or MkDocs (modern).

### Research Questions
1. Which tool is better for Python API docs?
   - Sphinx: Traditional, powerful, complex
   - MkDocs: Modern, simple, Markdown-based
2. Do we want API docs and user docs in same site or separate?
3. What hosting? (ReadTheDocs, GitHub Pages)
4. How to integrate with existing markdown docs?

### How to Verify

**Test Sphinx:**
```bash
pip install sphinx sphinx-rtd-theme sphinx-autodoc-typehints
sphinx-quickstart docs/
# Configure autodoc
make html
```

**Test MkDocs:**
```bash
pip install mkdocs mkdocs-material mkdocstrings[python]
mkdocs new .
# Configure mkdocstrings
mkdocs serve
```

**Compare:**
- Setup complexity
- Output quality
- Customization options
- Hosting integration

**Recommendation:**
- Sphinx: If we want traditional Python docs look (like stdlib)
- MkDocs: If we want modern, searchable, integrated docs

### Risk if Wrong
- May choose tool that doesn't meet needs
- Could require migration later
- Documentation quality suffers

### Estimated Research Time
2-3 hours (test both, compare, decide)

### Verification Results
‚úÖ **Status:** RESOLVED - Sphinx chosen for API documentation

**Decision:** Use Sphinx for API documentation
- Better autodoc support for Python projects
- NumPy/SciPy ecosystem compatibility
- Standard tool for Python API references
- Good integration with ReadTheDocs/GitHub Pages

**Rationale:**
- Sphinx is the de facto standard for Python API docs
- Excellent autodoc and type hint support
- User docs (tutorial, FAQ) remain in Markdown
- API reference gets its own Sphinx site

**Completed:** Sprint 5 Planning (Day 9 Task 9.5)

---

## Unknown 5.2: What topics are most critical for the tutorial?

### Priority
**Medium** - Content planning

### Assumption
Tutorial should cover installation ‚Üí first MCP ‚Üí understanding output ‚Üí troubleshooting, in that order.

### Research Questions
1. What's the learning path for new users?
2. What prerequisites should we assume? (Python knowledge, GAMS knowledge, optimization knowledge)
3. How detailed should examples be?
4. What common mistakes to highlight?

### How to Verify

**Outline potential tutorial structure:**

1. **Introduction** (5 min read)
   - What is nlp2mcp?
   - When to use it?
   - How does NLP ‚Üí KKT ‚Üí MCP work?

2. **Installation** (5 min)
   - `pip install nlp2mcp`
   - Verify installation
   - Optional: GAMS/PATH setup

3. **First Conversion** (15 min)
   - Simple example model
   - Run `nlp2mcp example.gms -o output.gms`
   - Examine output
   - Solve with PATH

4. **Understanding the Output** (15 min)
   - Stationarity equations
   - Complementarity pairs
   - Multiplier variables
   - Model declaration

5. **Common Patterns** (20 min)
   - Bounds handling
   - Inequality constraints
   - Indexed variables
   - Parameters

6. **Troubleshooting** (15 min)
   - Parse errors
   - Unsupported features
   - PATH convergence issues
   - When to use scaling

**User testing:**
- Ask someone unfamiliar with tool to follow tutorial
- Note where they get stuck
- Refine based on feedback

### Risk if Wrong
- Tutorial too advanced or too basic
- Users give up early
- Support burden increases

### Estimated Research Time
2 hours (outline, draft first version)

### Verification Results
üîç **Status:** INCOMPLETE - Can refine during Priority 5

---

## Unknown 5.3: How detailed should the troubleshooting guide be?

### Priority
**Medium** - Balance comprehensiveness vs maintainability

### Assumption
Troubleshooting guide should cover the top 10-15 most common issues with clear diagnostic steps and solutions.

### Research Questions
1. What are the most common user issues? (Based on Sprint 4 experience)
2. How much detail per issue? (One paragraph vs full diagnostic procedure)
3. Should we include:
   - Error message reference (all possible errors)?
   - Decision trees (flowcharts)?
   - Example fixes (code snippets)?
4. How to keep it updated as tool evolves?

### How to Verify

**Collect common issues:**

From Sprint 4 testing:
1. "Parse error: unexpected token"
2. "Variable not found in any equation"
3. "PATH solver: Model Status 5"
4. "GAMS error: no ref to var in equ.var"
5. "Unsupported function: abs()"
6. "Division by zero in derivative"
7. "NaN in Jacobian"
8. "Model too large, runs out of memory"

**For each issue, document:**
- Symptom (error message or behavior)
- Cause (what user did wrong)
- Diagnostic steps (how to confirm cause)
- Solution (how to fix)
- Prevention (how to avoid)

**Example format:**
```markdown
### Issue: "Parse error: unexpected token"

**Symptom:** Error message during parsing phase

**Common Causes:**
1. GAMS syntax not supported by nlp2mcp subset
2. Typo in model file
3. Missing semicolon

**Diagnostic Steps:**
1. Check error message for line number
2. Compare syntax with supported subset (see USER_GUIDE.md)
3. Try parsing with GAMS directly

**Solution:**
- If unsupported syntax: Rewrite using supported features
- If typo: Fix typo
- If subset limitation: File feature request

**Prevention:** Validate GAMS model compiles before converting
```

### Risk if Wrong
- Guide too detailed (overwhelming)
- Guide too sparse (not helpful)
- Maintenance burden

### Estimated Research Time
2-3 hours (draft guide, test usability)

### Verification Results
üîç **Status:** INCOMPLETE - Can refine during Priority 5

---

## Unknown 5.4: Should API reference include internal functions or only public API?

### Priority
**Low** - Scope decision

### Assumption
API reference should document public functions (CLI, main pipeline functions) but not internal implementation details.

### Research Questions
1. What constitutes "public API"?
   - CLI entry point (`nlp2mcp` command)?
   - High-level functions (`parse_model_file`, `compute_derivatives`, `emit_gams_mcp`)?
   - IR data structures?
   - Everything in `src/`?
2. Do advanced users need internal docs?
3. How to mark functions as public vs private? (naming convention, `__all__`)

### How to Verify

**Define API layers:**

**Layer 1: CLI** (end users)
- `nlp2mcp` command
- All CLI flags

**Layer 2: High-level API** (Python library users)
```python
from nlp2mcp import parse_model_file, compute_derivatives, assemble_kkt_system, emit_gams_mcp
```

**Layer 3: IR and data structures** (advanced users, extension developers)
```python
from nlp2mcp.ir import ModelIR, VariableDef, EquationDef
from nlp2mcp.ast import Expr, Binary, VarRef
```

**Layer 4: Internal** (developers only)
- Everything else with leading underscore
- Not documented in public API reference

**Recommendation:**
- Document Layers 1-3 in API reference
- Provide developer guide separately for Layer 4

### Risk if Wrong
- Too much documentation (confusing)
- Too little documentation (frustrating for advanced users)

### Estimated Research Time
1 hour (decide scope, mark public functions)

### Verification Results
üîç **Status:** INCOMPLETE - Low priority decision

---

# Summary and Recommendations

## Critical Path Unknowns (Must Resolve Before Sprint 5 Start)

1. **Unknown 1.1** (Critical): Strategy 2 coverage - 3-4 hours
2. **Unknown 1.4** (Critical): KKT assembly fix - 3-4 hours
3. **Unknown 2.1** (High): PATH failures diagnostic - 3-4 hours
4. **Unknown 3.1** (High): Performance targets - 3-4 hours
5. **Unknown 4.1** (High): Build system choice - 2 hours

**Total Critical Path Research: 14-18 hours**

## Pre-Sprint 5 Research Schedule

**Week 1 (Pre-Sprint 5):**
- Days 1-2: Unknowns 1.1, 1.4 (min/max fix research) - 6-8 hours
- Day 3: Unknown 2.1 (PATH failures) - 3-4 hours
- Day 4: Unknown 3.1 (performance targets) - 3-4 hours
- Day 5: Unknown 4.1 (build system) + Unknown 5.1 (docs tool) - 4-5 hours

**Total: ~16-21 hours spread over 1 week**

## Unknowns That Can Be Resolved During Sprint 5

- Unknown 1.2, 1.3, 1.5: During Priority 1 implementation
- Unknown 2.2, 2.3, 2.4: During Priority 2 implementation
- Unknown 3.2, 3.3, 3.4, 3.5: During Priority 3 implementation
- Unknown 4.2, 4.3, 4.4: During Priority 4 implementation
- Unknown 5.2, 5.3, 5.4: During Priority 5 implementation

## Risk Mitigation

**If Critical Unknowns Take Longer:**
- Priority 1 (min/max fix) has fallback: detect and error (1-2 hour implementation)
- Priority 2 (PATH) has fallback: document known issues, defer full validation
- Priority 3 (hardening) can be scoped down to fewer edge cases
- Priority 4 (packaging) can use default choices (setuptools, basic metadata)
- Priority 5 (docs) can use existing USER_GUIDE.md + minimal additions

**Checkpoint Strategy:**
- Day 3: Check Priority 1 and 2 complete
- Day 6: Check Priority 3 and 4 on track
- Day 9: Final integration check

---

**Document Status:** üîç DRAFT - Ready for pre-sprint research phase  
**Next Steps:** Begin resolving Critical Path unknowns (Week 1 schedule above)  
**Success Metric:** All Critical/High unknowns resolved before Sprint 5 Day 1
